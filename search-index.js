var searchIndex = JSON.parse('{\
"thermite":{"doc":"","t":[18,13,13,6,3,3,3,18,16,18,18,18,18,3,18,13,13,13,8,8,8,8,8,8,8,8,8,8,4,8,8,8,8,8,8,8,8,8,16,3,3,6,16,6,16,16,6,16,6,16,6,16,16,6,16,6,16,6,16,11,11,11,11,11,11,11,11,11,11,11,0,11,11,11,11,11,0,11,11,11,11,10,11,11,11,11,11,10,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,10,11,11,11,11,11,23,14,11,0,10,10,11,11,11,11,11,11,10,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,0,11,11,11,11,10,11,11,11,11,11,11,10,11,11,11,11,11,11,11,0,11,10,10,11,10,10,10,10,11,11,11,11,10,11,11,11,11,10,10,10,10,11,11,11,11,11,11,11,11,11,10,11,10,11,11,11,11,11,10,11,11,10,0,11,10,11,10,10,11,11,11,10,10,11,11,11,11,11,11,11,11,14,11,11,10,11,11,10,11,11,11,11,10,11,11,11,11,10,11,11,11,11,10,11,10,11,11,11,11,11,11,10,11,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,10,11,11,10,0,0,0,0,0,0,0,0,0,0,3,3,3,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,3,3,3,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,0,0,0,0,0,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,5,8,8,8,10,3,3,16,8,16,3,8,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,13,13,6,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,18,8,3,4,13,10,10,10,8,8,8,10,13,10,10,10,10,10,10,10,10,10,10,10,10,10,10,12,11,11,11,11,11,11,11,11,11,10,10,12,11,11,11,0,10,10,10,10,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,10,10,11,11,10,10,10,10,10,10,10,10,10,10,10,10,12,10,10,11,0,0,10,10,10,10,10,10,10,10,10,10,10,10,12,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,12,3,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,3,3,3,3,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,8,11,11,11,11,0,10,0,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,3,3,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11],"n":["ALIGNMENT","AVX","AVX2","AssociatedVector","BitMask","BranchfreeDivider","Divider","ELEMENT_SIZE","Element","FULL_BITMASK","FULL_BITMASK","INDICES","INSTRSET","Mask","NUM_ELEMENTS","SSE2","SSE42","Scalar","Simd","SimdAssociatedVector","SimdBitsFrom","SimdBitsInto","SimdBitwise","SimdCastTo","SimdCasts","SimdFloatVector","SimdFromBits","SimdFromCast","SimdInstructionSet","SimdIntVector","SimdIntegerDivision","SimdIntoBits","SimdPointer","SimdShuffleIndices","SimdSignedVector","SimdUnsignedIntVector","SimdVector","SimdVectorBase","V","VPtr","VectorBuffer","Vf32","Vf32","Vf64","Vf64","Vi","Vi32","Vi32","Vi64","Vi64","Visize","Visize","Vu","Vu32","Vu32","Vu64","Vu64","Vusize","Vusize","abs","add","all","all","alloc","alloc","and_not","and_not","any","any","approx_eq","arch","as_mut_slice","as_mut_vector_slice","as_ord_int","as_slice","as_vector_slice","backends","bitand","bitand","bitand_assign","bitand_assign","bitmask","bitmask","bitor","bitor","bitor_assign","bitor_assign","bits_from","bits_from","bits_into","bits_into","bitxor","bitxor","bitxor_assign","bitxor_assign","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast","cast","cast","cast","cast","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_to","cast_to","ceil","clamp","clone","clone","clone","clone","clone","clone","clone_into","clone_into","clone_into","clone_into","clone_into","clone_into","cmp","combine_sign","conditional_add","conditional_neg","conditional_sub","copysign","count","count","count_ones","count_zeros","default","deref","deref","deref_mut","dispatch","dispatch_dyn","drop","element","epsilon","eq","eq","eq","eq","eq","eq","extract","extract_unchecked","extract_unchecked","falsey","falsey","fill","floor","fmt","fmt","fmt","fmt","fmt","fract","from","from","from","from","from","from","from","from_bitmask","from_bits","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_value","gather","gather","gather_masked","gather_masked_unchecked","gather_unchecked","ge","gt","has_emulated_fma","has_true_fma","hash","i16","i32","i64","i8","indexed","infinity","into","into","into","into","into","into","into","into_bits","is_finite","is_infinite","is_nan","is_negative","is_normal","is_null","is_positive","is_power_of_two","is_subnormal","is_zero_or_subnormal","iter","iter_vectors","iter_vectors_mut","le","leading_ones","leading_zeros","len","len_vectors","load_aligned","load_aligned_unchecked","load_aligned_unchecked","load_f16_unaligned","load_f16_unaligned_unchecked","load_unaligned","load_unaligned_unchecked","load_unaligned_unchecked","load_vector","log2p1","lt","map_scalar","math","max","max_element","max_value","min","min_element","min_positive","min_value","mul_add","mul_adde","mul_sub","mul_sube","multiplier","nan","ne","ne","ne","ne","neg_infinity","neg_one","neg_zero","next_power_of_two_m1","nmul_add","nmul_adde","nmul_sub","nmul_sube","none","none","not","not","num_registers","one","partial_cmp","product","raw","rcp","read","read_masked","replace","replace_unchecked","replace_unchecked","reverse","reverse_bits","rng","rol","rolv","ror","rorv","round","rsqrt","runtime_detect","saturate","saturating_add","saturating_sub","scatter","scatter_masked","scatter_masked_unchecked","scatter_unchecked","select","select_negative","shift","shuffle","shuffle","shuffle_dyn","shuffle_dyn_unchecked","shuffle_unchecked","shuffle_unchecked","signum","splat","splat","splat","splat_any","splat_as","sqrt","store_aligned","store_aligned_unchecked","store_aligned_unchecked","store_f16_unaligned","store_f16_unaligned_unchecked","store_unaligned","store_unaligned_unchecked","store_unaligned_unchecked","store_vector","sum","swap","to_int_fast","to_owned","to_owned","to_owned","to_owned","to_owned","to_owned","to_uint_fast","trailing_ones","trailing_zeros","trunc","truthy","truthy","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","type_id","type_id","u16","u16","u16_branchfree","u32","u32","u32_branchfree","u64","u64","u64_branchfree","u8","u8","u8_branchfree","undefined","value","wrapping_product","wrapping_sum","write","write_masked","zero","avx","avx2","f16c","fma","sse","sse2","sse3","sse41","sse42","ssse3","__m256","__m256d","__m256i","_mm256_add_pd","_mm256_add_ps","_mm256_addsub_pd","_mm256_addsub_ps","_mm256_and_pd","_mm256_and_ps","_mm256_andnot_pd","_mm256_andnot_ps","_mm256_blend_pd","_mm256_blend_ps","_mm256_blendv_pd","_mm256_blendv_ps","_mm256_broadcast_pd","_mm256_broadcast_ps","_mm256_broadcast_sd","_mm256_broadcast_ss","_mm256_castpd128_pd256","_mm256_castpd256_pd128","_mm256_castpd_ps","_mm256_castpd_si256","_mm256_castps128_ps256","_mm256_castps256_ps128","_mm256_castps_pd","_mm256_castps_si256","_mm256_castsi128_si256","_mm256_castsi256_pd","_mm256_castsi256_ps","_mm256_castsi256_si128","_mm256_ceil_pd","_mm256_ceil_ps","_mm256_cmp_pd","_mm256_cmp_ps","_mm256_cvtepi32_pd","_mm256_cvtepi32_ps","_mm256_cvtpd_epi32","_mm256_cvtpd_ps","_mm256_cvtps_epi32","_mm256_cvtps_pd","_mm256_cvtsd_f64","_mm256_cvtsi256_si32","_mm256_cvtss_f32","_mm256_cvttpd_epi32","_mm256_cvttps_epi32","_mm256_div_pd","_mm256_div_ps","_mm256_dp_ps","_mm256_extract_epi32","_mm256_extract_epi64","_mm256_extractf128_pd","_mm256_extractf128_ps","_mm256_extractf128_si256","_mm256_floor_pd","_mm256_floor_ps","_mm256_hadd_pd","_mm256_hadd_ps","_mm256_hsub_pd","_mm256_hsub_ps","_mm256_insert_epi16","_mm256_insert_epi32","_mm256_insert_epi64","_mm256_insert_epi8","_mm256_insertf128_pd","_mm256_insertf128_ps","_mm256_insertf128_si256","_mm256_lddqu_si256","_mm256_load_pd","_mm256_load_ps","_mm256_load_si256","_mm256_loadu2_m128","_mm256_loadu2_m128d","_mm256_loadu2_m128i","_mm256_loadu_pd","_mm256_loadu_ps","_mm256_loadu_si256","_mm256_maskload_pd","_mm256_maskload_ps","_mm256_maskstore_pd","_mm256_maskstore_ps","_mm256_max_pd","_mm256_max_ps","_mm256_min_pd","_mm256_min_ps","_mm256_movedup_pd","_mm256_movehdup_ps","_mm256_moveldup_ps","_mm256_movemask_pd","_mm256_movemask_ps","_mm256_mul_pd","_mm256_mul_ps","_mm256_or_pd","_mm256_or_ps","_mm256_permute2f128_pd","_mm256_permute2f128_ps","_mm256_permute2f128_si256","_mm256_permute_pd","_mm256_permute_ps","_mm256_permutevar_pd","_mm256_permutevar_ps","_mm256_rcp_ps","_mm256_round_pd","_mm256_round_ps","_mm256_rsqrt_ps","_mm256_set1_epi16","_mm256_set1_epi32","_mm256_set1_epi64x","_mm256_set1_epi8","_mm256_set1_pd","_mm256_set1_ps","_mm256_set_epi16","_mm256_set_epi32","_mm256_set_epi64x","_mm256_set_epi8","_mm256_set_m128","_mm256_set_m128d","_mm256_set_m128i","_mm256_set_pd","_mm256_set_ps","_mm256_setr_epi16","_mm256_setr_epi32","_mm256_setr_epi64x","_mm256_setr_epi8","_mm256_setr_m128","_mm256_setr_m128d","_mm256_setr_m128i","_mm256_setr_pd","_mm256_setr_ps","_mm256_setzero_pd","_mm256_setzero_ps","_mm256_setzero_si256","_mm256_shuffle_pd","_mm256_shuffle_ps","_mm256_sqrt_pd","_mm256_sqrt_ps","_mm256_store_pd","_mm256_store_ps","_mm256_store_si256","_mm256_storeu2_m128","_mm256_storeu2_m128d","_mm256_storeu2_m128i","_mm256_storeu_pd","_mm256_storeu_ps","_mm256_storeu_si256","_mm256_stream_pd","_mm256_stream_ps","_mm256_stream_si256","_mm256_sub_pd","_mm256_sub_ps","_mm256_testc_pd","_mm256_testc_ps","_mm256_testc_si256","_mm256_testnzc_pd","_mm256_testnzc_ps","_mm256_testnzc_si256","_mm256_testz_pd","_mm256_testz_ps","_mm256_testz_si256","_mm256_undefined_pd","_mm256_undefined_ps","_mm256_undefined_si256","_mm256_unpackhi_pd","_mm256_unpackhi_ps","_mm256_unpacklo_pd","_mm256_unpacklo_ps","_mm256_xor_pd","_mm256_xor_ps","_mm256_zeroall","_mm256_zeroupper","_mm256_zextpd128_pd256","_mm256_zextps128_ps256","_mm256_zextsi128_si256","_mm_broadcast_ss","_mm_cmp_pd","_mm_cmp_ps","_mm_cmp_sd","_mm_cmp_ss","_mm_maskload_pd","_mm_maskload_ps","_mm_maskstore_pd","_mm_maskstore_ps","_mm_permute_pd","_mm_permute_ps","_mm_permutevar_pd","_mm_permutevar_ps","_mm_testc_pd","_mm_testc_ps","_mm_testnzc_pd","_mm_testnzc_ps","_mm_testz_pd","_mm_testz_ps","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast_mask","cast_mask","cast_mask","clone","clone","clone","clone_into","clone_into","clone_into","fmt","fmt","fmt","from","from","from","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","into","into","into","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id","_mm256_abs_epi16","_mm256_abs_epi32","_mm256_abs_epi8","_mm256_add_epi16","_mm256_add_epi32","_mm256_add_epi64","_mm256_add_epi8","_mm256_adds_epi16","_mm256_adds_epi8","_mm256_adds_epu16","_mm256_adds_epu8","_mm256_alignr_epi8","_mm256_and_si256","_mm256_andnot_si256","_mm256_avg_epu16","_mm256_avg_epu8","_mm256_blend_epi16","_mm256_blend_epi32","_mm256_blendv_epi8","_mm256_broadcastb_epi8","_mm256_broadcastd_epi32","_mm256_broadcastq_epi64","_mm256_broadcastsd_pd","_mm256_broadcastsi128_si256","_mm256_broadcastss_ps","_mm256_broadcastw_epi16","_mm256_bslli_epi128","_mm256_bsrli_epi128","_mm256_cmpeq_epi16","_mm256_cmpeq_epi32","_mm256_cmpeq_epi64","_mm256_cmpeq_epi8","_mm256_cmpgt_epi16","_mm256_cmpgt_epi32","_mm256_cmpgt_epi64","_mm256_cmpgt_epi8","_mm256_cvtepi16_epi32","_mm256_cvtepi16_epi64","_mm256_cvtepi32_epi64","_mm256_cvtepi8_epi16","_mm256_cvtepi8_epi32","_mm256_cvtepi8_epi64","_mm256_cvtepu16_epi32","_mm256_cvtepu16_epi64","_mm256_cvtepu32_epi64","_mm256_cvtepu8_epi16","_mm256_cvtepu8_epi32","_mm256_cvtepu8_epi64","_mm256_extract_epi16","_mm256_extract_epi8","_mm256_extracti128_si256","_mm256_hadd_epi16","_mm256_hadd_epi32","_mm256_hadds_epi16","_mm256_hsub_epi16","_mm256_hsub_epi32","_mm256_hsubs_epi16","_mm256_i32gather_epi32","_mm256_i32gather_epi64","_mm256_i32gather_pd","_mm256_i32gather_ps","_mm256_i64gather_epi32","_mm256_i64gather_epi64","_mm256_i64gather_pd","_mm256_i64gather_ps","_mm256_inserti128_si256","_mm256_madd_epi16","_mm256_maddubs_epi16","_mm256_mask_i32gather_epi32","_mm256_mask_i32gather_epi64","_mm256_mask_i32gather_pd","_mm256_mask_i32gather_ps","_mm256_mask_i64gather_epi32","_mm256_mask_i64gather_epi64","_mm256_mask_i64gather_pd","_mm256_mask_i64gather_ps","_mm256_maskload_epi32","_mm256_maskload_epi64","_mm256_maskstore_epi32","_mm256_maskstore_epi64","_mm256_max_epi16","_mm256_max_epi32","_mm256_max_epi8","_mm256_max_epu16","_mm256_max_epu32","_mm256_max_epu8","_mm256_min_epi16","_mm256_min_epi32","_mm256_min_epi8","_mm256_min_epu16","_mm256_min_epu32","_mm256_min_epu8","_mm256_movemask_epi8","_mm256_mpsadbw_epu8","_mm256_mul_epi32","_mm256_mul_epu32","_mm256_mulhi_epi16","_mm256_mulhi_epu16","_mm256_mulhrs_epi16","_mm256_mullo_epi16","_mm256_mullo_epi32","_mm256_or_si256","_mm256_packs_epi16","_mm256_packs_epi32","_mm256_packus_epi16","_mm256_packus_epi32","_mm256_permute2x128_si256","_mm256_permute4x64_epi64","_mm256_permute4x64_pd","_mm256_permutevar8x32_epi32","_mm256_permutevar8x32_ps","_mm256_sad_epu8","_mm256_shuffle_epi32","_mm256_shuffle_epi8","_mm256_shufflehi_epi16","_mm256_shufflelo_epi16","_mm256_sign_epi16","_mm256_sign_epi32","_mm256_sign_epi8","_mm256_sll_epi16","_mm256_sll_epi32","_mm256_sll_epi64","_mm256_slli_epi16","_mm256_slli_epi32","_mm256_slli_epi64","_mm256_slli_si256","_mm256_sllv_epi32","_mm256_sllv_epi64","_mm256_sra_epi16","_mm256_sra_epi32","_mm256_srai_epi16","_mm256_srai_epi32","_mm256_srav_epi32","_mm256_srl_epi16","_mm256_srl_epi32","_mm256_srl_epi64","_mm256_srli_epi16","_mm256_srli_epi32","_mm256_srli_epi64","_mm256_srli_si256","_mm256_srlv_epi32","_mm256_srlv_epi64","_mm256_sub_epi16","_mm256_sub_epi32","_mm256_sub_epi64","_mm256_sub_epi8","_mm256_subs_epi16","_mm256_subs_epi8","_mm256_subs_epu16","_mm256_subs_epu8","_mm256_unpackhi_epi16","_mm256_unpackhi_epi32","_mm256_unpackhi_epi64","_mm256_unpackhi_epi8","_mm256_unpacklo_epi16","_mm256_unpacklo_epi32","_mm256_unpacklo_epi64","_mm256_unpacklo_epi8","_mm256_xor_si256","_mm_blend_epi32","_mm_broadcastb_epi8","_mm_broadcastd_epi32","_mm_broadcastq_epi64","_mm_broadcastsd_pd","_mm_broadcastss_ps","_mm_broadcastw_epi16","_mm_i32gather_epi32","_mm_i32gather_epi64","_mm_i32gather_pd","_mm_i32gather_ps","_mm_i64gather_epi32","_mm_i64gather_epi64","_mm_i64gather_pd","_mm_i64gather_ps","_mm_mask_i32gather_epi32","_mm_mask_i32gather_epi64","_mm_mask_i32gather_pd","_mm_mask_i32gather_ps","_mm_mask_i64gather_epi32","_mm_mask_i64gather_epi64","_mm_mask_i64gather_pd","_mm_mask_i64gather_ps","_mm_maskload_epi32","_mm_maskload_epi64","_mm_maskstore_epi32","_mm_maskstore_epi64","_mm_sllv_epi32","_mm_sllv_epi64","_mm_srav_epi32","_mm_srlv_epi32","_mm_srlv_epi64","_mm256_fmadd_pd","_mm256_fmadd_ps","_mm256_fmaddsub_pd","_mm256_fmaddsub_ps","_mm256_fmsub_pd","_mm256_fmsub_ps","_mm256_fmsubadd_pd","_mm256_fmsubadd_ps","_mm256_fnmadd_pd","_mm256_fnmadd_ps","_mm256_fnmsub_pd","_mm256_fnmsub_ps","_mm_fmadd_pd","_mm_fmadd_ps","_mm_fmadd_sd","_mm_fmadd_ss","_mm_fmaddsub_pd","_mm_fmaddsub_ps","_mm_fmsub_pd","_mm_fmsub_ps","_mm_fmsub_sd","_mm_fmsub_ss","_mm_fmsubadd_pd","_mm_fmsubadd_ps","_mm_fnmadd_pd","_mm_fnmadd_ps","_mm_fnmadd_sd","_mm_fnmadd_ss","_mm_fnmsub_pd","_mm_fnmsub_ps","_mm_fnmsub_sd","_mm_fnmsub_ss","_CMP_EQ_OQ","_CMP_EQ_OS","_CMP_EQ_UQ","_CMP_EQ_US","_CMP_FALSE_OQ","_CMP_FALSE_OS","_CMP_GE_OQ","_CMP_GE_OS","_CMP_GT_OQ","_CMP_GT_OS","_CMP_LE_OQ","_CMP_LE_OS","_CMP_LT_OQ","_CMP_LT_OS","_CMP_NEQ_OQ","_CMP_NEQ_OS","_CMP_NEQ_UQ","_CMP_NEQ_US","_CMP_NGE_UQ","_CMP_NGE_US","_CMP_NGT_UQ","_CMP_NGT_US","_CMP_NLE_UQ","_CMP_NLE_US","_CMP_NLT_UQ","_CMP_NLT_US","_CMP_ORD_Q","_CMP_ORD_S","_CMP_TRUE_UQ","_CMP_TRUE_US","_CMP_UNORD_Q","_CMP_UNORD_S","_MM_FROUND_CEIL","_MM_FROUND_CUR_DIRECTION","_MM_FROUND_FLOOR","_MM_FROUND_NEARBYINT","_MM_FROUND_NINT","_MM_FROUND_NO_EXC","_MM_FROUND_RAISE_EXC","_MM_FROUND_RINT","_MM_FROUND_TO_NEAREST_INT","_MM_FROUND_TO_NEG_INF","_MM_FROUND_TO_POS_INF","_MM_FROUND_TO_ZERO","_MM_FROUND_TRUNC","__m128","__m128d","__m128i","_mm_add_ps","_mm_add_ss","_mm_and_ps","_mm_andnot_ps","_mm_cmpeq_ps","_mm_cmpeq_ss","_mm_cmpge_ps","_mm_cmpge_ss","_mm_cmpgt_ps","_mm_cmpgt_ss","_mm_cmple_ps","_mm_cmple_ss","_mm_cmplt_ps","_mm_cmplt_ss","_mm_cmpneq_ps","_mm_cmpneq_ss","_mm_cmpnge_ps","_mm_cmpnge_ss","_mm_cmpngt_ps","_mm_cmpngt_ss","_mm_cmpnle_ps","_mm_cmpnle_ss","_mm_cmpnlt_ps","_mm_cmpnlt_ss","_mm_cmpord_ps","_mm_cmpord_ss","_mm_cmpunord_ps","_mm_cmpunord_ss","_mm_comieq_ss","_mm_comige_ss","_mm_comigt_ss","_mm_comile_ss","_mm_comilt_ss","_mm_comineq_ss","_mm_cvt_si2ss","_mm_cvt_ss2si","_mm_cvtsi32_ss","_mm_cvtsi64_ss","_mm_cvtss_f32","_mm_cvtss_si32","_mm_cvtss_si64","_mm_cvtt_ss2si","_mm_cvttss_si32","_mm_cvttss_si64","_mm_div_ps","_mm_div_ss","_mm_load1_ps","_mm_load_ps","_mm_load_ps1","_mm_load_ss","_mm_loadr_ps","_mm_loadu_ps","_mm_loadu_si64","_mm_max_ps","_mm_max_ss","_mm_min_ps","_mm_min_ss","_mm_move_ss","_mm_movehl_ps","_mm_movelh_ps","_mm_movemask_ps","_mm_mul_ps","_mm_mul_ss","_mm_or_ps","_mm_prefetch","_mm_rcp_ps","_mm_rcp_ss","_mm_rsqrt_ps","_mm_rsqrt_ss","_mm_set1_ps","_mm_set_ps","_mm_set_ps1","_mm_set_ss","_mm_setcsr","_mm_setr_ps","_mm_setzero_ps","_mm_sfence","_mm_shuffle_ps","_mm_sqrt_ps","_mm_sqrt_ss","_mm_store1_ps","_mm_store_ps","_mm_store_ps1","_mm_store_ss","_mm_storer_ps","_mm_storeu_ps","_mm_stream_ps","_mm_sub_ps","_mm_sub_ss","_mm_ucomieq_ss","_mm_ucomige_ss","_mm_ucomigt_ss","_mm_ucomile_ss","_mm_ucomilt_ss","_mm_ucomineq_ss","_mm_undefined_ps","_mm_unpackhi_ps","_mm_unpacklo_ps","_mm_xor_ps","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast_mask","cast_mask","cast_mask","clone","clone","clone","clone_into","clone_into","clone_into","fmt","fmt","fmt","from","from","from","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","into","into","into","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id","_mm_add_epi16","_mm_add_epi32","_mm_add_epi64","_mm_add_epi8","_mm_add_pd","_mm_add_sd","_mm_adds_epi16","_mm_adds_epi8","_mm_adds_epu16","_mm_adds_epu8","_mm_and_pd","_mm_and_si128","_mm_andnot_pd","_mm_andnot_si128","_mm_avg_epu16","_mm_avg_epu8","_mm_bslli_si128","_mm_bsrli_si128","_mm_castpd_ps","_mm_castpd_si128","_mm_castps_pd","_mm_castps_si128","_mm_castsi128_pd","_mm_castsi128_ps","_mm_clflush","_mm_cmpeq_epi16","_mm_cmpeq_epi32","_mm_cmpeq_epi8","_mm_cmpeq_pd","_mm_cmpeq_sd","_mm_cmpge_pd","_mm_cmpge_sd","_mm_cmpgt_epi16","_mm_cmpgt_epi32","_mm_cmpgt_epi8","_mm_cmpgt_pd","_mm_cmpgt_sd","_mm_cmple_pd","_mm_cmple_sd","_mm_cmplt_epi16","_mm_cmplt_epi32","_mm_cmplt_epi8","_mm_cmplt_pd","_mm_cmplt_sd","_mm_cmpneq_pd","_mm_cmpneq_sd","_mm_cmpnge_pd","_mm_cmpnge_sd","_mm_cmpngt_pd","_mm_cmpngt_sd","_mm_cmpnle_pd","_mm_cmpnle_sd","_mm_cmpnlt_pd","_mm_cmpnlt_sd","_mm_cmpord_pd","_mm_cmpord_sd","_mm_cmpunord_pd","_mm_cmpunord_sd","_mm_comieq_sd","_mm_comige_sd","_mm_comigt_sd","_mm_comile_sd","_mm_comilt_sd","_mm_comineq_sd","_mm_cvtepi32_pd","_mm_cvtepi32_ps","_mm_cvtpd_epi32","_mm_cvtpd_ps","_mm_cvtps_epi32","_mm_cvtps_pd","_mm_cvtsd_f64","_mm_cvtsd_si32","_mm_cvtsd_ss","_mm_cvtsi128_si32","_mm_cvtsi128_si64","_mm_cvtsi128_si64x","_mm_cvtsi32_sd","_mm_cvtsi32_si128","_mm_cvtsi64x_si128","_mm_cvtss_sd","_mm_cvttpd_epi32","_mm_cvttps_epi32","_mm_cvttsd_si32","_mm_cvttsd_si64","_mm_cvttsd_si64x","_mm_div_pd","_mm_div_sd","_mm_extract_epi16","_mm_insert_epi16","_mm_lfence","_mm_load1_pd","_mm_load_pd","_mm_load_pd1","_mm_load_sd","_mm_load_si128","_mm_loadh_pd","_mm_loadl_epi64","_mm_loadl_pd","_mm_loadr_pd","_mm_loadu_pd","_mm_loadu_si128","_mm_madd_epi16","_mm_maskmoveu_si128","_mm_max_epi16","_mm_max_epu8","_mm_max_pd","_mm_max_sd","_mm_mfence","_mm_min_epi16","_mm_min_epu8","_mm_min_pd","_mm_min_sd","_mm_move_epi64","_mm_move_sd","_mm_movemask_epi8","_mm_movemask_pd","_mm_mul_epu32","_mm_mul_pd","_mm_mul_sd","_mm_mulhi_epi16","_mm_mulhi_epu16","_mm_mullo_epi16","_mm_or_pd","_mm_or_si128","_mm_packs_epi16","_mm_packs_epi32","_mm_packus_epi16","_mm_pause","_mm_sad_epu8","_mm_set1_epi16","_mm_set1_epi32","_mm_set1_epi64x","_mm_set1_epi8","_mm_set1_pd","_mm_set_epi16","_mm_set_epi32","_mm_set_epi64x","_mm_set_epi8","_mm_set_pd","_mm_set_pd1","_mm_set_sd","_mm_setr_epi16","_mm_setr_epi32","_mm_setr_epi8","_mm_setr_pd","_mm_setzero_pd","_mm_setzero_si128","_mm_shuffle_epi32","_mm_shuffle_pd","_mm_shufflehi_epi16","_mm_shufflelo_epi16","_mm_sll_epi16","_mm_sll_epi32","_mm_sll_epi64","_mm_slli_epi16","_mm_slli_epi32","_mm_slli_epi64","_mm_slli_si128","_mm_sqrt_pd","_mm_sqrt_sd","_mm_sra_epi16","_mm_sra_epi32","_mm_srai_epi16","_mm_srai_epi32","_mm_srl_epi16","_mm_srl_epi32","_mm_srl_epi64","_mm_srli_epi16","_mm_srli_epi32","_mm_srli_epi64","_mm_srli_si128","_mm_store1_pd","_mm_store_pd","_mm_store_pd1","_mm_store_sd","_mm_store_si128","_mm_storeh_pd","_mm_storel_epi64","_mm_storel_pd","_mm_storer_pd","_mm_storeu_pd","_mm_storeu_si128","_mm_stream_pd","_mm_stream_si128","_mm_stream_si32","_mm_stream_si64","_mm_sub_epi16","_mm_sub_epi32","_mm_sub_epi64","_mm_sub_epi8","_mm_sub_pd","_mm_sub_sd","_mm_subs_epi16","_mm_subs_epi8","_mm_subs_epu16","_mm_subs_epu8","_mm_ucomieq_sd","_mm_ucomige_sd","_mm_ucomigt_sd","_mm_ucomile_sd","_mm_ucomilt_sd","_mm_ucomineq_sd","_mm_undefined_pd","_mm_undefined_si128","_mm_unpackhi_epi16","_mm_unpackhi_epi32","_mm_unpackhi_epi64","_mm_unpackhi_epi8","_mm_unpackhi_pd","_mm_unpacklo_epi16","_mm_unpacklo_epi32","_mm_unpacklo_epi64","_mm_unpacklo_epi8","_mm_unpacklo_pd","_mm_xor_pd","_mm_xor_si128","_mm_addsub_pd","_mm_addsub_ps","_mm_hadd_pd","_mm_hadd_ps","_mm_hsub_pd","_mm_hsub_ps","_mm_lddqu_si128","_mm_loaddup_pd","_mm_movedup_pd","_mm_movehdup_ps","_mm_moveldup_ps","_mm_blend_epi16","_mm_blend_pd","_mm_blend_ps","_mm_blendv_epi8","_mm_blendv_pd","_mm_blendv_ps","_mm_ceil_pd","_mm_ceil_ps","_mm_ceil_sd","_mm_ceil_ss","_mm_cmpeq_epi64","_mm_cvtepi16_epi32","_mm_cvtepi16_epi64","_mm_cvtepi32_epi64","_mm_cvtepi8_epi16","_mm_cvtepi8_epi32","_mm_cvtepi8_epi64","_mm_cvtepu16_epi32","_mm_cvtepu16_epi64","_mm_cvtepu32_epi64","_mm_cvtepu8_epi16","_mm_cvtepu8_epi32","_mm_cvtepu8_epi64","_mm_dp_pd","_mm_dp_ps","_mm_extract_epi32","_mm_extract_epi64","_mm_extract_epi8","_mm_extract_ps","_mm_floor_pd","_mm_floor_ps","_mm_floor_sd","_mm_floor_ss","_mm_insert_epi32","_mm_insert_epi64","_mm_insert_epi8","_mm_insert_ps","_mm_max_epi32","_mm_max_epi8","_mm_max_epu16","_mm_max_epu32","_mm_min_epi32","_mm_min_epi8","_mm_min_epu16","_mm_min_epu32","_mm_minpos_epu16","_mm_mpsadbw_epu8","_mm_mul_epi32","_mm_mullo_epi32","_mm_packus_epi32","_mm_round_pd","_mm_round_ps","_mm_round_sd","_mm_round_ss","_mm_test_all_ones","_mm_test_all_zeros","_mm_test_mix_ones_zeros","_mm_testc_si128","_mm_testnzc_si128","_mm_testz_si128","_mm_cmpestra","_mm_cmpestrc","_mm_cmpestri","_mm_cmpestrm","_mm_cmpestro","_mm_cmpestrs","_mm_cmpestrz","_mm_cmpgt_epi64","_mm_cmpistra","_mm_cmpistrc","_mm_cmpistri","_mm_cmpistrm","_mm_cmpistro","_mm_cmpistrs","_mm_cmpistrz","_mm_crc32_u16","_mm_crc32_u32","_mm_crc32_u64","_mm_crc32_u8","_mm_abs_epi16","_mm_abs_epi32","_mm_abs_epi8","_mm_alignr_epi8","_mm_hadd_epi16","_mm_hadd_epi32","_mm_hadds_epi16","_mm_hsub_epi16","_mm_hsub_epi32","_mm_hsubs_epi16","_mm_maddubs_epi16","_mm_mulhrs_epi16","_mm_shuffle_epi8","_mm_sign_epi16","_mm_sign_epi32","_mm_sign_epi8","avx1","avx2","polyfills","sse2","sse42","AVX1","borrow","borrow_mut","cast","cast_mask","clone","clone_into","eq","fmt","from","from_cast","from_cast_mask","hash","into","to_owned","try_from","try_into","type_id","AVX2","_mm_add","_mm_add","_mm_add","_mm_add","_mm_add","_mm_add","_mm_bitand","_mm_bitand","_mm_bitand","_mm_bitand","_mm_bitand","_mm_bitand","_mm_bitor","_mm_bitor","_mm_bitor","_mm_bitor","_mm_bitor","_mm_bitor","_mm_bitxor","_mm_bitxor","_mm_bitxor","_mm_bitxor","_mm_bitxor","_mm_bitxor","_mm_div","_mm_div","_mm_div","_mm_div","_mm_div","_mm_div","_mm_mul","_mm_mul","_mm_mul","_mm_mul","_mm_mul","_mm_mul","_mm_neg","_mm_neg","_mm_neg","_mm_neg","_mm_not","_mm_not","_mm_not","_mm_not","_mm_not","_mm_not","_mm_rem","_mm_rem","_mm_rem","_mm_rem","_mm_rem","_mm_rem","_mm_shl","_mm_shl","_mm_shl","_mm_shl","_mm_shl","_mm_shl","_mm_shli","_mm_shli","_mm_shli","_mm_shli","_mm_shli","_mm_shli","_mm_shr","_mm_shr","_mm_shr","_mm_shr","_mm_shr","_mm_shr","_mm_shri","_mm_shri","_mm_shri","_mm_shri","_mm_shri","_mm_shri","_mm_sub","_mm_sub","_mm_sub","_mm_sub","_mm_sub","_mm_sub","abs","abs","abs","abs","add","add","add","add","add","add","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","add_assign","and_not","and_not","and_not","and_not","and_not","and_not","bitand","bitand","bitand","bitand","bitand","bitand","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitand_assign","bitmask","bitmask","bitmask","bitmask","bitmask","bitmask","bitor","bitor","bitor","bitor","bitor","bitor","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bitor_assign","bits_from","bits_from","bits_from","bits_from","bits_from","bits_from","bits_into","bits_into","bits_into","bits_into","bits_into","bits_into","bitxor","bitxor","bitxor","bitxor","bitxor","bitxor","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","bitxor_assign","borrow","borrow","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast","cast","cast","cast","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","ceil","ceil","clone","clone","clone","clone","clone","clone","clone","clone_into","clone_into","clone_into","clone_into","clone_into","clone_into","clone_into","conditional_neg","conditional_neg","conditional_neg","conditional_neg","copysign","copysign","count_ones","count_ones","count_ones","count_ones","default","default","default","default","default","default","div","div","div","div","div","div","div","div","div","div","div","div","div","div","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","div_assign","epsilon","epsilon","eq","eq","eq","eq","eq","eq","eq","eq","eq","eq","eq","eq","eq","extract_unchecked","extract_unchecked","extract_unchecked","extract_unchecked","extract_unchecked","extract_unchecked","f32x8","f64x8","floor","floor","fmt","fmt","fmt","fmt","fmt","fmt","fmt","from","from","from","from","from","from","from","from_bits","from_bits","from_bits","from_bits","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","gather_masked_unchecked","gather_masked_unchecked","gather_masked_unchecked","gather_masked_unchecked","gather_masked_unchecked","gather_masked_unchecked","gather_unchecked","gather_unchecked","gather_unchecked","gather_unchecked","gather_unchecked","gather_unchecked","ge","ge","ge","ge","ge","ge","gt","gt","gt","gt","gt","gt","hash","i32x8","i64x8","indexed","indexed","indexed","indexed","indexed","indexed","infinity","infinity","into","into","into","into","into","into","into","into_bits","into_bits","into_bits","into_bits","is_negative","is_negative","is_positive","is_positive","is_subnormal","is_subnormal","is_zero_or_subnormal","is_zero_or_subnormal","le","le","leading_zeros","leading_zeros","leading_zeros","leading_zeros","load_aligned_unchecked","load_aligned_unchecked","load_aligned_unchecked","load_aligned_unchecked","load_aligned_unchecked","load_aligned_unchecked","load_f16_unaligned_unchecked","load_f16_unaligned_unchecked","load_unaligned_unchecked","load_unaligned_unchecked","load_unaligned_unchecked","load_unaligned_unchecked","lt","lt","max","max","max","max","max_element","max_element","max_element","max_element","max_element","max_element","max_value","max_value","max_value","max_value","max_value","max_value","min","min","min","min","min_element","min_element","min_element","min_element","min_element","min_element","min_positive","min_positive","min_positive","min_positive","min_value","min_value","min_value","min_value","min_value","min_value","mul","mul","mul","mul","mul","mul","mul_add","mul_add","mul_adde","mul_adde","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_assign","mul_sub","mul_sub","mul_sube","mul_sube","nan","nan","ne","ne","ne","ne","neg","neg","neg","neg","neg_infinity","neg_infinity","neg_one","neg_one","neg_one","neg_one","neg_zero","neg_zero","next_power_of_two_m1","next_power_of_two_m1","nmul_add","nmul_add","nmul_adde","nmul_adde","nmul_sub","nmul_sub","nmul_sube","nmul_sube","not","not","not","not","not","not","one","one","one","one","one","one","product","product","rcp","rem","rem","rem","rem","rem","rem","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","rem_assign","replace_unchecked","replace_unchecked","replace_unchecked","replace_unchecked","replace_unchecked","replace_unchecked","reverse_bits","reverse_bits","reverse_bits","reverse_bits","rolv","rolv","rolv","rolv","rorv","rorv","rorv","rorv","round","round","rsqrt","saturating_add","saturating_add","saturating_add","saturating_add","saturating_sub","saturating_sub","saturating_sub","saturating_sub","select_negative","select_negative","select_negative","select_negative","shl","shl","shl","shl","shl","shl","shl","shl","shl","shl","shl","shl","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shl_assign","shr","shr","shr","shr","shr","shr","shr","shr","shr","shr","shr","shr","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shr_assign","shuffle_unchecked","shuffle_unchecked","shuffle_unchecked","shuffle_unchecked","shuffle_unchecked","shuffle_unchecked","signum","signum","splat","splat","splat","splat","splat","splat","sqrt","sqrt","store_aligned_unchecked","store_aligned_unchecked","store_aligned_unchecked","store_aligned_unchecked","store_aligned_unchecked","store_aligned_unchecked","store_f16_unaligned_unchecked","store_f16_unaligned_unchecked","store_unaligned_unchecked","store_unaligned_unchecked","store_unaligned_unchecked","store_unaligned_unchecked","sub","sub","sub","sub","sub","sub","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sub_assign","sum","sum","to_int_fast","to_int_fast","to_owned","to_owned","to_owned","to_owned","to_owned","to_owned","to_owned","to_uint_fast","to_uint_fast","trailing_zeros","trailing_zeros","trailing_zeros","trailing_zeros","trunc","trunc","try_from","try_from","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","type_id","type_id","u32x8","u64x8","undefined","undefined","undefined","undefined","undefined","undefined","wrapping_product","wrapping_product","wrapping_product","wrapping_product","wrapping_sum","wrapping_sum","wrapping_sum","wrapping_sum","zero","zero","zero","zero","zero","zero","_mm_shuffle","CastFrom","CastFromAll","SimdElement","cast_from","AlignedMut","AlignedMutIter","IntoIter","IntoSimdIterator","Item","SimdCastIter","SimdIteratorExt","SimdSliceIter","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast","cast","cast_mask","cast_mask","cast_mask","cast_mask","clone","clone","clone_into","clone_into","from","from","from","from","from_cast","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","into","into","into","into","into_iter","into_iter","into_iter","into_simd_iter","iter_mut","new","new","new_unchecked","next","next","next","remainder","remainder","size_hint","size_hint","store","store","store","to_owned","to_owned","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","Average","Best","DefaultPolicy","E","FRAC_1_PI","FRAC_1_SQRT_2","FRAC_1_SQRT_PI","FRAC_2_PI","FRAC_2_SQRT_PI","FRAC_PI_2","FRAC_PI_3","FRAC_PI_4","FRAC_PI_6","FRAC_PI_8","LN_10","LN_2","LN_PI","LOG10_2","LOG10_E","LOG2_10","LOG2_E","PI","POLICY","Policy","PolicyParameters","PrecisionPolicy","Reference","SQRT_2","SQRT_E","SQRT_FRAC_PI_2","SimdFloatVectorConsts","SimdVectorizedMath","SimdVectorizedMathPolicied","TAU","Worst","acos","acos_p","acosh","acosh_p","asin","asin_p","asinh","asinh_p","atan","atan2","atan2_p","atan_p","atanh","atanh_p","avoid_branching","avoid_precision_branches","borrow","borrow","borrow_mut","borrow_mut","cast","cast","cast_mask","cast_mask","cbrt","cbrt_p","check_overflow","clone","clone_into","cmp","compensated","cos","cos_p","cosh","cosh_p","eq","erf","erf_p","erfinv","erfinv_p","exp","exp10","exp10_p","exp2","exp2_p","exp_m1","exp_m1_p","exp_p","exph","exph_p","fmod","fmod_p","fmt","from","from","from_cast","from_cast","from_cast_mask","from_cast_mask","hash","hypot","hypot_p","into","into","invsqrt","invsqrt_p","lerp","lerp_p","ln","ln_1p","ln_1p_p","ln_p","log10","log10_p","log2","log2_p","max_series_iterations","next_float","next_float_p","partial_cmp","policies","poly","poly","poly_f","poly_f_p","poly_p","poly_rational","poly_rational_p","powf","powf_p","powi","powi_p","powiv","powiv_p","precision","prev_float","prev_float_p","product_f","product_f_p","reciprocal","reciprocal_p","scale","scale_p","sin","sin_cos","sin_cos_p","sin_p","sinh","sinh_p","smootherstep","smootherstep_p","smootheststep","smootheststep_p","smoothstep","smoothstep_p","sum_f","sum_f_p","summation_f","summation_f_p","tan","tan_p","tanh","tanh_p","to_owned","try_from","try_from","try_into","try_into","type_id","type_id","unroll_loops","Compensated","add","borrow","borrow_mut","cast","cast_mask","clone","clone_into","err","fmt","from","from_cast","from_cast_mask","into","mul","new","product","sum","to_owned","try_from","try_into","type_id","val","value","ExtraPrecision","Performance","Precision","Reference","Size","UltraPerformance","borrow","borrow","borrow","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast","cast","cast","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","cast_mask","clone","clone","clone","clone","clone","clone","clone_into","clone_into","clone_into","clone_into","clone_into","clone_into","cmp","cmp","cmp","cmp","cmp","cmp","eq","eq","eq","eq","eq","eq","fmt","fmt","fmt","fmt","fmt","fmt","from","from","from","from","from","from","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","from_cast_mask","hash","hash","hash","hash","hash","hash","into","into","into","into","into","into","ne","partial_cmp","partial_cmp","partial_cmp","partial_cmp","partial_cmp","partial_cmp","to_owned","to_owned","to_owned","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_from","try_from","try_from","try_into","try_into","try_into","try_into","try_into","try_into","type_id","type_id","type_id","type_id","type_id","type_id","poly_1","poly_10","poly_11","poly_12","poly_13","poly_14","poly_15","poly_2","poly_3","poly_30","poly_4","poly_5","poly_6","poly_7","poly_8","poly_9","SimdRng","next_f32","next_f64","next_u32","next_u64","pcg32","reseed","xoshiro","PCG32","borrow","borrow_mut","cast","cast_mask","clone","clone_into","eq","fmt","from","from_cast","from_cast_mask","into","ne","new","next_u32","reseed","to_owned","try_from","try_into","type_id","SplitMix64","Xoshiro128Plus","Xoshiro256Plus","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","cast","cast","cast","cast_mask","cast_mask","cast_mask","clone","clone","clone","clone_into","clone_into","clone_into","eq","eq","eq","fmt","fmt","fmt","from","from","from","from_cast","from_cast","from_cast","from_cast_mask","from_cast_mask","from_cast_mask","into","into","into","ne","ne","ne","new","new","new","next_u32","next_u64","next_u64","next_u64","reseed","reseed","reseed","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id"],"q":["thermite","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch","","","","","","","","","","thermite::arch::avx","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::avx2","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::fma","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::sse","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::sse2","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::sse3","","","","","","","","","","","thermite::arch::sse41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::arch::sse42","","","","","","","","","","","","","","","","","","","thermite::arch::ssse3","","","","","","","","","","","","","","","","thermite::backends","","","","","thermite::backends::avx1","","","","","","","","","","","","","","","","","","thermite::backends::avx2","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::backends::polyfills","thermite::element","","","","thermite::iter","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::math","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::math::compensated","","","","","","","","","","","","","","","","","","","","","","","","thermite::math::policies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","thermite::math::poly","","","","","","","","","","","","","","","","thermite::rng","","","","","","","","thermite::rng::pcg32","","","","","","","","","","","","","","","","","","","","","thermite::rng::xoshiro","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"d":["","","","Associated vector type for a scalar type","Small integer representation of a mask using individual ","Divider without branching, useful for dynamic divisors.","Divider recommended for constant divisors.","Size of element type in bytes","","Bitmask corresponding to all lanes of the mask being ","Bitmask indicating all truthy values for each lane","","","","","","","","SIMD Instruction set, contains all types","","Reflexive helper trait to overcome trait bounds cycles","Reflexive helper trait to overcome trait bounds cycles","Defines bitwise operations on vectors","Describes casting to one SIMD vector type from another","List of valid casts between SIMD types in an instruction ","Floating point SIMD vectors","Transmutations from raw bits","Describes casting from one SIMD vector type to another","Enum of supported instruction sets","Integer SIMD vectors","Specialized integer division by [<code>Divider</code>]s","Transmutations into raw bits","Guarantees the vector can be used as a pointer in <code>VPtr</code>","Helper trait for constant vector shuffles","Signed SIMD vector, with negative numbers","Unsigned SIMD vector","Defines common operations on numeric vectors","Basic shared vector interface","","","Aligned SIMD vector storage","","Single-precision 32-bit floating point vector","","Double-precision 64-bit floating point vector","","","32-bit signed integer vector","","64-bit signed integer vector","","","","","32-bit unsigned integer vector","","64-bit unsigned integer vector","","","Absolute value","","Return true if all lanes for this vector type are set to ","Returns <code>true</code> if all lanes are truthy","Allocates a new SIMD-aligned element buffer and zeroes ","","Computes <code>!self & other</code>, may be more performant than the ","Computes <code>!self & other</code> for each lane of the mask.","Return true if any lane for this vector type is set to ","Returns <code>true</code> if any lanes are truthy","","Organized target-feature intrinsics","","","Converts floating-point values to integer values that can ","","","","","","","","Returns an integer where each bit corresponds to the ","Returns a bitmask that can be trivially evaluated to ","","","","","","","","","","","","","","","","","","","","","","","","","","","Casts one vector to another, performing proper numeric ","","","","","","","","Casts one mask to another, not caring about the value ","","","","","","","","","","Rounds upwards towards positive infinity.","","","","","","","","","","","","","","","Same as <code>self * sign.signum()</code> or ","Add <code>self</code> and <code>value</code> only if the corresponding lane in the ","For each lane, if the mask is true, negate the value.","Subtracts <code>value</code> from <code>self</code> only if the corresponding lane ","Copies the sign from <code>sign</code> to <code>self</code>","Count the number of set bits","Counts the number of truthy lanes","Counts the number of 1 bits in each lane of the vector.","Counts the number of 0 bits in each lane of the vector.","","","","","Generates monomorphized backend <code>target_feature</code> function ","Detects processor architecture at runtime and generates a ","","","","","","","","","","Extracts an element at the given lane index.","Extracts an element at the given lane index.","","Create a bitmask with all false bits","Mask vector containing all zero/false lanes.","Fills the buffer with vectors using aligned stores","Rounds downward towards negative infinity.","","","","","","Returns the fractional part of a number (the part between ","","","","","","","","Creates a wide SIMD mask from a single-bit bitmask","","Casts one vector to another, performing proper numeric ","","","","","","","","Casts one mask to another, not caring about the value ","","","","","","","","Creates a mask from a value. Any non-zero lanes are ","Gathers values from the buffer using more efficient ","Loads values from arbitrary addresses in memory based on ","","Like <code>Self::gather</code>, but individual lanes are loaded based ","","","","On older platforms, fused multiply-add instructions can ","True fused multiply-add instructions are only used on ","","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","Returns a vector where the first lane is zero, and each ","","","","","","","","","","","","","Test if negative, less than zero","","","Test if positive, greater or equal to zero","Returns a mask wherein if a lane was a power of two, the ","","","","","","","Counts the number of leading ones in each lane of the ","Counts the number of leading zeros in each lane of the ","","","Loads a vector from a slice that has an alignment of at ","Loads a vector from the given aligned address.","","Load half-precision floats and up-convert them into <code>Self</code>","","Loads a vector from a slice","Loads a vector from a given address (does not have to be ","","","Returns <code>floor(log2(x)) + 1</code>","","Maps each lane of the vector to a new vector using a ","Vectorized Math Library","Per-lane, select the maximum value","Find the maximum value across all lanes","Minimum representable valid value (may be negative)","Per-lane, select the minimum value","Find the minimum value across all lanes","Minimum positive number","Maximum representable valid value","Fused multiply-add","Fused multiply-add, with <em>at worst</em> precision equal to ","Fused multiply-subtract","Fused multiply-subtract, with <em>at worst</em> precision equal to ","","","","","","","","","","Returns <code>next_power_of_two(x) - 1</code>","Fused negated multiply-add","Fused negated multiply-add, with <em>at worst</em> precision equal ","Fused negated multiply-subtract","Fused negated multiply-subtract, with <em>at worst</em> precision ","Return true if no lanes for this vector type are set to ","Returns <code>true</code> if all lanes are falsey","","","The number of general-purpose registers that can be ","","","Compute the horizontal product of all elements","","Computes the approximate reciprocal/inverse of each value","","","Returns a new vector with the given value at the given ","Returns a new vector with the given value at the given ","","Reverses the elements in the vector","Reverses the bits of each lane in the vector.","","Rotates the bits in each lane to the left (towards HSB) ","Rotates the bits in each lane to the left (towards HSB) ","Rotates the bits in each lane to the right (towards LSB) ","Rotates the bits in each lane to the right (towards LSB) ","Rounds to the nearest representable integer.","Compute the approximate reciprocal of the square root ","","Clamps self to between 0 and 1","Saturating addition, will not wrap","Saturating subtraction, will not wrap","Stores values to arbitrary addresses in memory based on ","","Like <code>self.scatter()</code>, but individual lanes are stored ","","For each lane, selects from <code>t</code> if the mask lane is truthy, ","On platforms with true select instructions, they ","","Shuffles between two vectors based on the static indices ","Shuffles the elements in one or two vectors into a new ","Shuffles between two vectors based on the dynamic indices ","Like [<code>Self::shuffle_dyn</code>], but does not check for valid ","Shuffles between two vectors based on the static indices ","","Returns <code>-1</code> if less than zero, <code>+1</code> otherwise.","Creates a new vector with all lanes set to the given value","","","Same as <code>splat</code>, but is more convenient for initializing ","Splats a value by casting to the element type via ","Calculates the square-root of each element in the vector.","Stores a vector into a slice with an alignment of at ","Stores a vector to the given aligned address.","","Down-convert <code>self</code> into half-precision and store","","Stores a vector into a slice.","Stores a vector to a given address (does not have to be ","","","Compute the horizontal sum of all elements","For each lane, if the mask lane is truthy then swap the ","Can convert to a signed integer faster than a regular <code>cast</code>","","","","","","","Can convert to an unsigned integer faster than a regular ","Counts the number of trailing ones in each lane of the ","Counts the number of trailing zeros in each lane of the ","Truncates any rational value towards zero","Create a bitmask for this vector type of all true bits","Mask vector containing all true/non-zero lanes.","","","","","","","","","","","","","","","","","","","","","","See docs for [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","See docs for [<code>BranchfreeDivider</code>] and [<code>Divider</code>]","Returns a vector containing possibly undefined or ","","Multiply all lanes together, wrapping the result if it can","Sum all lanes together, wrapping the result if it cant ","","","","","","","","","","","","","","256-bit wide set of eight <code>f32</code> types, x86-specific","256-bit wide set of four <code>f64</code> types, x86-specific","256-bit wide integer vector type, x86-specific","Adds packed double-precision (64-bit) floating-point ","Adds packed single-precision (32-bit) floating-point ","Alternatively adds and subtracts packed double-precision ","Alternatively adds and subtracts packed single-precision ","Computes the bitwise AND of a packed double-precision ","Computes the bitwise AND of packed single-precision ","Computes the bitwise NOT of packed double-precision ","Computes the bitwise NOT of packed single-precision ","Blends packed double-precision (64-bit) floating-point ","Blends packed single-precision (32-bit) floating-point ","Blends packed double-precision (64-bit) floating-point ","Blends packed single-precision (32-bit) floating-point ","Broadcasts 128 bits from memory (composed of 2 packed ","Broadcasts 128 bits from memory (composed of 4 packed ","Broadcasts a double-precision (64-bit) floating-point ","Broadcasts a single-precision (32-bit) floating-point ","Casts vector of type __m128d to type __m256d; the upper ","Casts vector of type __m256d to type __m128d.","Cast vector of type __m256d to type __m256.","Casts vector of type __m256d to type __m256i.","Casts vector of type __m128 to type __m256; the upper 128 ","Casts vector of type __m256 to type __m128.","Cast vector of type __m256 to type __m256d.","Casts vector of type __m256 to type __m256i.","Casts vector of type __m128i to type __m256i; the upper ","Casts vector of type __m256i to type __m256d.","Casts vector of type __m256i to type __m256.","Casts vector of type __m256i to type __m128i.","Rounds packed double-precision (64-bit) floating point ","Rounds packed single-precision (32-bit) floating point ","Compares packed double-precision (64-bit) floating-point ","Compares packed single-precision (32-bit) floating-point ","Converts packed 32-bit integers in <code>a</code> to packed ","Converts packed 32-bit integers in <code>a</code> to packed ","Converts packed double-precision (64-bit) floating-point ","Converts packed double-precision (64-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Returns the first element of the input vector of ","Returns the first element of the input vector of <code>[8 x i32]</code>","Returns the first element of the input vector of ","Converts packed double-precision (64-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Computes the division of each of the 4 packed 64-bit ","Computes the division of each of the 8 packed 32-bit ","Conditionally multiplies the packed single-precision ","Extracts a 32-bit integer from <code>a</code>, selected with <code>imm8</code>.","Extracts a 64-bit integer from <code>a</code>, selected with <code>imm8</code>.","Extracts 128 bits (composed of 2 packed double-precision ","Extracts 128 bits (composed of 4 packed single-precision ","Extracts 128 bits (composed of integer data) from <code>a</code>, ","Rounds packed double-precision (64-bit) floating point ","Rounds packed single-precision (32-bit) floating point ","Horizontal addition of adjacent pairs in the two packed ","Horizontal addition of adjacent pairs in the two packed ","Horizontal subtraction of adjacent pairs in the two ","Horizontal subtraction of adjacent pairs in the two ","Copies <code>a</code> to result, and inserts the 16-bit integer <code>i</code> into ","Copies <code>a</code> to result, and inserts the 32-bit integer <code>i</code> into ","Copies <code>a</code> to result, and insert the 64-bit integer <code>i</code> into ","Copies <code>a</code> to result, and inserts the 8-bit integer <code>i</code> into ","Copies <code>a</code> to result, then inserts 128 bits (composed of 2 ","Copies <code>a</code> to result, then inserts 128 bits (composed of 4 ","Copies <code>a</code> to result, then inserts 128 bits from <code>b</code> into ","Loads 256-bits of integer data from unaligned memory into ","Loads 256-bits (composed of 4 packed double-precision ","Loads 256-bits (composed of 8 packed single-precision ","Loads 256-bits of integer data from memory into result. ","Loads two 128-bit values (composed of 4 packed ","Loads two 128-bit values (composed of 2 packed ","Loads two 128-bit values (composed of integer data) from ","Loads 256-bits (composed of 4 packed double-precision ","Loads 256-bits (composed of 8 packed single-precision ","Loads 256-bits of integer data from memory into result. ","Loads packed double-precision (64-bit) floating-point ","Loads packed single-precision (32-bit) floating-point ","Stores packed double-precision (64-bit) floating-point ","Stores packed single-precision (32-bit) floating-point ","Compares packed double-precision (64-bit) floating-point ","Compares packed single-precision (32-bit) floating-point ","Compares packed double-precision (64-bit) floating-point ","Compares packed single-precision (32-bit) floating-point ","Duplicate even-indexed double-precision (64-bit) ","Duplicate odd-indexed single-precision (32-bit) ","Duplicate even-indexed single-precision (32-bit) ","Sets each bit of the returned mask based on the most ","Sets each bit of the returned mask based on the most ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Computes the bitwise OR packed double-precision (64-bit) ","Computes the bitwise OR packed single-precision (32-bit) ","Shuffles 256 bits (composed of 4 packed double-precision ","Shuffles 256 bits (composed of 8 packed single-precision ","Shuffles 128-bits (composed of integer data) selected by ","Shuffles double-precision (64-bit) floating-point ","Shuffles single-precision (32-bit) floating-point ","Shuffles double-precision (64-bit) floating-point ","Shuffles single-precision (32-bit) floating-point ","Computes the approximate reciprocal of packed ","Rounds packed double-precision (64-bit) floating point ","Rounds packed single-precision (32-bit) floating point ","Computes the approximate reciprocal square root of packed ","Broadcasts 16-bit integer <code>a</code> to all all elements of ","Broadcasts 32-bit integer <code>a</code> to all elements of returned ","Broadcasts 64-bit integer <code>a</code> to all elements of returned ","Broadcasts 8-bit integer <code>a</code> to all elements of returned ","Broadcasts double-precision (64-bit) floating-point value ","Broadcasts single-precision (32-bit) floating-point value ","Sets packed 16-bit integers in returned vector with the ","Sets packed 32-bit integers in returned vector with the ","Sets packed 64-bit integers in returned vector with the ","Sets packed 8-bit integers in returned vector with the ","Sets packed __m256 returned vector with the supplied ","Sets packed __m256d returned vector with the supplied ","Sets packed __m256i returned vector with the supplied ","Sets packed double-precision (64-bit) floating-point ","Sets packed single-precision (32-bit) floating-point ","Sets packed 16-bit integers in returned vector with the ","Sets packed 32-bit integers in returned vector with the ","Sets packed 64-bit integers in returned vector with the ","Sets packed 8-bit integers in returned vector with the ","Sets packed __m256 returned vector with the supplied ","Sets packed __m256d returned vector with the supplied ","Sets packed __m256i returned vector with the supplied ","Sets packed double-precision (64-bit) floating-point ","Sets packed single-precision (32-bit) floating-point ","Returns vector of type __m256d with all elements set to ","Returns vector of type __m256 with all elements set to ","Returns vector of type __m256i with all elements set to ","Shuffles double-precision (64-bit) floating-point ","Shuffles single-precision (32-bit) floating-point ","Returns the square root of packed double-precision ","Returns the square root of packed single-precision ","Stores 256-bits (composed of 4 packed double-precision ","Stores 256-bits (composed of 8 packed single-precision ","Stores 256-bits of integer data from <code>a</code> into memory. ","Stores the high and low 128-bit halves (each composed of ","Stores the high and low 128-bit halves (each composed of ","Stores the high and low 128-bit halves (each composed of ","Stores 256-bits (composed of 4 packed double-precision ","Stores 256-bits (composed of 8 packed single-precision ","Stores 256-bits of integer data from <code>a</code> into memory. ","Moves double-precision values from a 256-bit vector of ","Moves single-precision floating point values from a ","Moves integer data from a 256-bit integer vector to a ","Subtracts packed double-precision (64-bit) floating-point ","Subtracts packed single-precision (32-bit) floating-point ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise AND of 256 bits (representing ","Returns vector of type <code>__m256d</code> with undefined elements.","Returns vector of type <code>__m256</code> with undefined elements.","Returns vector of type __m256i with undefined elements.","Unpacks and interleave double-precision (64-bit) ","Unpacks and interleave single-precision (32-bit) ","Unpacks and interleave double-precision (64-bit) ","Unpacks and interleave single-precision (32-bit) ","Computes the bitwise XOR of packed double-precision ","Computes the bitwise XOR of packed single-precision ","Zeroes the contents of all XMM or YMM registers.","Zeroes the upper 128 bits of all YMM registers; the lower ","Constructs a 256-bit floating-point vector of <code>[4 x double]</code>","Constructs a 256-bit floating-point vector of <code>[8 x float]</code> ","Constructs a 256-bit integer vector from a 128-bit ","Broadcasts a single-precision (32-bit) floating-point ","Compares packed double-precision (64-bit) floating-point ","Compares packed single-precision (32-bit) floating-point ","Compares the lower double-precision (64-bit) ","Compares the lower single-precision (32-bit) ","Loads packed double-precision (64-bit) floating-point ","Loads packed single-precision (32-bit) floating-point ","Stores packed double-precision (64-bit) floating-point ","Stores packed single-precision (32-bit) floating-point ","Shuffles double-precision (64-bit) floating-point ","Shuffles single-precision (32-bit) floating-point ","Shuffles double-precision (64-bit) floating-point ","Shuffles single-precision (32-bit) floating-point ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise AND of 128 bits (representing ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Computes the absolute values of packed 16-bit integers in ","Computes the absolute values of packed 32-bit integers in ","Computes the absolute values of packed 8-bit integers in <code>a</code>","Adds packed 16-bit integers in <code>a</code> and <code>b</code>.","Adds packed 32-bit integers in <code>a</code> and <code>b</code>.","Adds packed 64-bit integers in <code>a</code> and <code>b</code>.","Adds packed 8-bit integers in <code>a</code> and <code>b</code>.","Adds packed 16-bit integers in <code>a</code> and <code>b</code> using saturation.","Adds packed 8-bit integers in <code>a</code> and <code>b</code> using saturation.","Adds packed unsigned 16-bit integers in <code>a</code> and <code>b</code> using ","Adds packed unsigned 8-bit integers in <code>a</code> and <code>b</code> using ","Concatenates pairs of 16-byte blocks in <code>a</code> and <code>b</code> into a ","Computes the bitwise AND of 256 bits (representing ","Computes the bitwise NOT of 256 bits (representing ","Averages packed unsigned 16-bit integers in <code>a</code> and <code>b</code>.","Averages packed unsigned 8-bit integers in <code>a</code> and <code>b</code>.","Blends packed 16-bit integers from <code>a</code> and <code>b</code> using control ","Blends packed 32-bit integers from <code>a</code> and <code>b</code> using control ","Blends packed 8-bit integers from <code>a</code> and <code>b</code> using <code>mask</code>.","Broadcasts the low packed 8-bit integer from <code>a</code> to all ","Broadcasts the low packed 32-bit integer from <code>a</code> to all ","Broadcasts the low packed 64-bit integer from <code>a</code> to all ","Broadcasts the low double-precision (64-bit) ","Broadcasts 128 bits of integer data from a to all 128-bit ","Broadcasts the low single-precision (32-bit) ","Broadcasts the low packed 16-bit integer from a to all ","Shifts 128-bit lanes in <code>a</code> left by <code>imm8</code> bytes while ","Shifts 128-bit lanes in <code>a</code> right by <code>imm8</code> bytes while ","Compares packed 16-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 32-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 64-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 8-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 16-bit integers in <code>a</code> and <code>b</code> for ","Compares packed 32-bit integers in <code>a</code> and <code>b</code> for ","Compares packed 64-bit integers in <code>a</code> and <code>b</code> for ","Compares packed 8-bit integers in <code>a</code> and <code>b</code> for ","Sign-extend 16-bit integers to 32-bit integers.","Sign-extend 16-bit integers to 64-bit integers.","Sign-extend 32-bit integers to 64-bit integers.","Sign-extend 8-bit integers to 16-bit integers.","Sign-extend 8-bit integers to 32-bit integers.","Sign-extend 8-bit integers to 64-bit integers.","Zeroes extend packed unsigned 16-bit integers in <code>a</code> to ","Zero-extend the lower four unsigned 16-bit integers in <code>a</code> ","Zero-extend unsigned 32-bit integers in <code>a</code> to 64-bit ","Zero-extend unsigned 8-bit integers in <code>a</code> to 16-bit ","Zero-extend the lower eight unsigned 8-bit integers in <code>a</code> ","Zero-extend the lower four unsigned 8-bit integers in <code>a</code> ","Extracts a 16-bit integer from <code>a</code>, selected with <code>imm8</code>. ","Extracts an 8-bit integer from <code>a</code>, selected with <code>imm8</code>. ","Extracts 128 bits (of integer data) from <code>a</code> selected with ","Horizontally adds adjacent pairs of 16-bit integers in <code>a</code> ","Horizontally adds adjacent pairs of 32-bit integers in <code>a</code> ","Horizontally adds adjacent pairs of 16-bit integers in <code>a</code> ","Horizontally subtract adjacent pairs of 16-bit integers ","Horizontally subtract adjacent pairs of 32-bit integers ","Horizontally subtract adjacent pairs of 16-bit integers ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Copies <code>a</code> to <code>dst</code>, then insert 128 bits (of integer data) ","Multiplies packed signed 16-bit integers in <code>a</code> and <code>b</code>, ","Vertically multiplies each unsigned 8-bit integer from <code>a</code> ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Loads packed 32-bit integers from memory pointed by ","Loads packed 64-bit integers from memory pointed by ","Stores packed 32-bit integers from <code>a</code> into memory pointed ","Stores packed 64-bit integers from <code>a</code> into memory pointed ","Compares packed 16-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 32-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 8-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed unsigned 16-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 32-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 8-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed 16-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 32-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 8-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed unsigned 16-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 32-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 8-bit integers in <code>a</code> and <code>b</code>, and ","Creates mask from the most significant bit of each 8-bit ","Computes the sum of absolute differences (SADs) of ","Multiplies the low 32-bit integers from each packed ","Multiplies the low unsigned 32-bit integers from each ","Multiplies the packed 16-bit integers in <code>a</code> and <code>b</code>, ","Multiplies the packed unsigned 16-bit integers in <code>a</code> and <code>b</code>","Multiplies packed 16-bit integers in <code>a</code> and <code>b</code>, producing ","Multiplies the packed 16-bit integers in <code>a</code> and <code>b</code>, ","Multiplies the packed 32-bit integers in <code>a</code> and <code>b</code>, ","Computes the bitwise OR of 256 bits (representing integer ","Converts packed 16-bit integers from <code>a</code> and <code>b</code> to packed ","Converts packed 32-bit integers from <code>a</code> and <code>b</code> to packed ","Converts packed 16-bit integers from <code>a</code> and <code>b</code> to packed ","Converts packed 32-bit integers from <code>a</code> and <code>b</code> to packed ","Shuffles 128-bits of integer data selected by <code>imm8</code> from <code>a</code> ","Permutes 64-bit integers from <code>a</code> using control mask <code>imm8</code>.","Shuffles 64-bit floating-point elements in <code>a</code> across lanes ","Permutes packed 32-bit integers from <code>a</code> according to the ","Shuffles eight 32-bit foating-point elements in <code>a</code> across ","Computes the absolute differences of packed unsigned ","Shuffles 32-bit integers in 128-bit lanes of <code>a</code> using the ","Shuffles bytes from <code>a</code> according to the content of <code>b</code>.","Shuffles 16-bit integers in the high 64 bits of 128-bit ","Shuffles 16-bit integers in the low 64 bits of 128-bit ","Negates packed 16-bit integers in <code>a</code> when the ","Negates packed 32-bit integers in <code>a</code> when the ","Negates packed 8-bit integers in <code>a</code> when the corresponding ","Shifts packed 16-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 64-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts packed 64-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts 128-bit lanes in <code>a</code> left by <code>imm8</code> bytes while ","Shifts packed 32-bit integers in <code>a</code> left by the amount ","Shifts packed 64-bit integers in <code>a</code> left by the amount ","Shifts packed 16-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> right by the amount ","Shifts packed 16-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 64-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 64-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts 128-bit lanes in <code>a</code> right by <code>imm8</code> bytes while ","Shifts packed 32-bit integers in <code>a</code> right by the amount ","Shifts packed 64-bit integers in <code>a</code> right by the amount ","Subtract packed 16-bit integers in <code>b</code> from packed 16-bit ","Subtract packed 32-bit integers in <code>b</code> from packed 32-bit ","Subtract packed 64-bit integers in <code>b</code> from packed 64-bit ","Subtract packed 8-bit integers in <code>b</code> from packed 8-bit ","Subtract packed 16-bit integers in <code>b</code> from packed 16-bit ","Subtract packed 8-bit integers in <code>b</code> from packed 8-bit ","Subtract packed unsigned 16-bit integers in <code>b</code> from packed ","Subtract packed unsigned 8-bit integers in <code>b</code> from packed ","Unpacks and interleave 16-bit integers from the high half ","Unpacks and interleave 32-bit integers from the high half ","Unpacks and interleave 64-bit integers from the high half ","Unpacks and interleave 8-bit integers from the high half ","Unpacks and interleave 16-bit integers from the low half ","Unpacks and interleave 32-bit integers from the low half ","Unpacks and interleave 64-bit integers from the low half ","Unpacks and interleave 8-bit integers from the low half ","Computes the bitwise XOR of 256 bits (representing ","Blends packed 32-bit integers from <code>a</code> and <code>b</code> using control ","Broadcasts the low packed 8-bit integer from <code>a</code> to all ","Broadcasts the low packed 32-bit integer from <code>a</code> to all ","Broadcasts the low packed 64-bit integer from <code>a</code> to all ","Broadcasts the low double-precision (64-bit) ","Broadcasts the low single-precision (32-bit) ","Broadcasts the low packed 16-bit integer from a to all ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Returns values from <code>slice</code> at offsets determined by ","Loads packed 32-bit integers from memory pointed by ","Loads packed 64-bit integers from memory pointed by ","Stores packed 32-bit integers from <code>a</code> into memory pointed ","Stores packed 64-bit integers from <code>a</code> into memory pointed ","Shifts packed 32-bit integers in <code>a</code> left by the amount ","Shifts packed 64-bit integers in <code>a</code> left by the amount ","Shifts packed 32-bit integers in <code>a</code> right by the amount ","Shifts packed 32-bit integers in <code>a</code> right by the amount ","Shifts packed 64-bit integers in <code>a</code> right by the amount ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies the lower double-precision (64-bit) ","Multiplies the lower single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies the lower double-precision (64-bit) ","Multiplies the lower single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies the lower double-precision (64-bit) ","Multiplies the lower single-precision (32-bit) ","Multiplies packed double-precision (64-bit) ","Multiplies packed single-precision (32-bit) ","Multiplies the lower double-precision (64-bit) ","Multiplies the lower single-precision (32-bit) ","Equal (ordered, non-signaling)","Equal (ordered, signaling)","Equal (unordered, non-signaling)","Equal (unordered, signaling)","False (ordered, non-signaling)","False (ordered, signaling)","Greater-than-or-equal (ordered, non-signaling)","Greater-than-or-equal (ordered, signaling)","Greater-than (ordered, non-signaling)","Greater-than (ordered, signaling)","Less-than-or-equal (ordered, non-signaling)","Less-than-or-equal (ordered, signaling)","Less-than (ordered, non-signaling)","Less-than (ordered, signaling)","Not-equal (ordered, non-signaling)","Not-equal (ordered, signaling)","Not-equal (unordered, non-signaling)","Not-equal (unordered, signaling)","Not-greater-than-or-equal (unordered, non-signaling)","Not-greater-than-or-equal (unordered, signaling)","Not-greater-than (unordered, non-signaling)","Not-greater-than (unordered, signaling)","Not-less-than-or-equal (unordered, non-signaling)","Not-less-than-or-equal (unordered, signaling)","Not-less-than (unordered, non-signaling)","Not-less-than (unordered, signaling)","Ordered (non-signaling)","Ordered (signaling)","True (unordered, non-signaling)","True (unordered, signaling)","Unordered (non-signaling)","Unordered (signaling)","round up and do not suppress exceptions","use MXCSR.RC; see <code>vendor::_MM_SET_ROUNDING_MODE</code>","round down and do not suppress exceptions","use MXCSR.RC and suppress exceptions; see ","round to nearest and do not suppress exceptions","suppress exceptions","do not suppress exceptions","use MXCSR.RC and do not suppress exceptions; see ","round to nearest","round down","round up","truncate","truncate and do not suppress exceptions","128-bit wide set of four <code>f32</code> types, x86-specific","128-bit wide set of two <code>f64</code> types, x86-specific","128-bit wide integer vector type, x86-specific","Adds __m128 vectors.","Adds the first component of <code>a</code> and <code>b</code>, the other components ","Bitwise AND of packed single-precision (32-bit) ","Bitwise AND-NOT of packed single-precision (32-bit) ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for equality. The ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for greater than ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for greater than. ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for less than or ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for less than. The ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for inequality. ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for ","Compares each of the four floats in <code>a</code> to the ","Compares the lowest <code>f32</code> of both inputs for not-less-than. ","Compares each of the four floats in <code>a</code> to the ","Checks if the lowest <code>f32</code> of both inputs are ordered. The ","Compares each of the four floats in <code>a</code> to the ","Checks if the lowest <code>f32</code> of both inputs are unordered. ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Alias for <code>_mm_cvtsi32_ss</code>.","Alias for <code>_mm_cvtss_si32</code>.","Converts a 32 bit integer to a 32 bit float. The result ","Converts a 64 bit integer to a 32 bit float. The result ","Extracts the lowest 32 bit float from the input vector.","Converts the lowest 32 bit float in the input vector to a ","Converts the lowest 32 bit float in the input vector to a ","Alias for <code>_mm_cvttss_si32</code>.","Converts the lowest 32 bit float in the input vector to a ","Converts the lowest 32 bit float in the input vector to a ","Divides __m128 vectors.","Divides the first component of <code>b</code> by <code>a</code>, the other ","Construct a <code>__m128</code> by duplicating the value read from <code>p</code> ","Loads four <code>f32</code> values from <em>aligned</em> memory into a <code>__m128</code>. ","Alias for <code>_mm_load1_ps</code>","Construct a <code>__m128</code> with the lowest element read from <code>p</code> ","Loads four <code>f32</code> values from aligned memory into a <code>__m128</code> ","Loads four <code>f32</code> values from memory into a <code>__m128</code>. There ","Loads unaligned 64-bits of integer data from memory into ","Compares packed single-precision (32-bit) floating-point ","Compares the first single-precision (32-bit) ","Compares packed single-precision (32-bit) floating-point ","Compares the first single-precision (32-bit) ","Returns a <code>__m128</code> with the first component from <code>b</code> and the ","Combine higher half of <code>a</code> and <code>b</code>. The highwe half of <code>b</code> ","Combine lower half of <code>a</code> and <code>b</code>. The lower half of <code>b</code> ","Returns a mask of the most significant bit of each ","Multiplies __m128 vectors.","Multiplies the first component of <code>a</code> and <code>b</code>, the other ","Bitwise OR of packed single-precision (32-bit) ","Fetch the cache line that contains address <code>p</code> using the ","Returns the approximate reciprocal of packed ","Returns the approximate reciprocal of the first ","Returns the approximate reciprocal square root of packed ","Returns the approximate reciprocal square root of the ","Construct a <code>__m128</code> with all element set to <code>a</code>.","Construct a <code>__m128</code> from four floating point values ","Alias for <code>_mm_set1_ps</code>","Construct a <code>__m128</code> with the lowest element set to <code>a</code> and ","Sets the MXCSR register with the 32-bit unsigned integer ","Construct a <code>__m128</code> from four floating point values lowest ","Construct a <code>__m128</code> with all elements initialized to zero.","Performs a serializing operation on all store-to-memory ","Shuffles packed single-precision (32-bit) floating-point ","Returns the square root of packed single-precision ","Returns the square root of the first single-precision ","Stores the lowest 32 bit float of <code>a</code> repeated four times ","Stores four 32-bit floats into <em>aligned</em> memory.","Alias for <code>_mm_store1_ps</code>","Stores the lowest 32 bit float of <code>a</code> into memory.","Stores four 32-bit floats into <em>aligned</em> memory in reverse ","Stores four 32-bit floats into memory. There are no ","Stores <code>a</code> into the memory at <code>mem_addr</code> using a non-temporal ","Subtracts __m128 vectors.","Subtracts the first component of <code>b</code> from <code>a</code>, the other ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Compares two 32-bit floats from the low-order bits of <code>a</code> ","Returns vector of type __m128 with undefined elements.","Unpacks and interleave single-precision (32-bit) ","Unpacks and interleave single-precision (32-bit) ","Bitwise exclusive OR of packed single-precision (32-bit) ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Adds packed 16-bit integers in <code>a</code> and <code>b</code>.","Adds packed 32-bit integers in <code>a</code> and <code>b</code>.","Adds packed 64-bit integers in <code>a</code> and <code>b</code>.","Adds packed 8-bit integers in <code>a</code> and <code>b</code>.","Adds packed double-precision (64-bit) floating-point ","Returns a new vector with the low element of <code>a</code> replaced ","Adds packed 16-bit integers in <code>a</code> and <code>b</code> using saturation.","Adds packed 8-bit integers in <code>a</code> and <code>b</code> using saturation.","Adds packed unsigned 16-bit integers in <code>a</code> and <code>b</code> using ","Adds packed unsigned 8-bit integers in <code>a</code> and <code>b</code> using ","Computes the bitwise AND of packed double-precision ","Computes the bitwise AND of 128 bits (representing ","Computes the bitwise NOT of <code>a</code> and then AND with <code>b</code>.","Computes the bitwise NOT of 128 bits (representing ","Averages packed unsigned 16-bit integers in <code>a</code> and <code>b</code>.","Averages packed unsigned 8-bit integers in <code>a</code> and <code>b</code>.","Shifts <code>a</code> left by <code>imm8</code> bytes while shifting in zeros.","Shifts <code>a</code> right by <code>imm8</code> bytes while shifting in zeros.","Casts a 128-bit floating-point vector of <code>[2 x double]</code> ","Casts a 128-bit floating-point vector of <code>[2 x double]</code> ","Casts a 128-bit floating-point vector of <code>[4 x float]</code> into ","Casts a 128-bit floating-point vector of <code>[4 x float]</code> into ","Casts a 128-bit integer vector into a 128-bit ","Casts a 128-bit integer vector into a 128-bit ","Invalidates and flushes the cache line that contains <code>p</code> ","Compares packed 16-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 32-bit integers in <code>a</code> and <code>b</code> for equality.","Compares packed 8-bit integers in <code>a</code> and <code>b</code> for equality.","Compares corresponding elements in <code>a</code> and <code>b</code> for equality.","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares packed 16-bit integers in <code>a</code> and <code>b</code> for ","Compares packed 32-bit integers in <code>a</code> and <code>b</code> for ","Compares packed 8-bit integers in <code>a</code> and <code>b</code> for ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares packed 16-bit integers in <code>a</code> and <code>b</code> for less-than.","Compares packed 32-bit integers in <code>a</code> and <code>b</code> for less-than.","Compares packed 8-bit integers in <code>a</code> and <code>b</code> for less-than.","Compares corresponding elements in <code>a</code> and <code>b</code> for less-than.","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for not-equal.","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> for ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> to see if ","Returns a new vector with the low element of <code>a</code> replaced ","Compares corresponding elements in <code>a</code> and <code>b</code> to see if ","Returns a new vector with the low element of <code>a</code> replaced ","Compares the lower element of <code>a</code> and <code>b</code> for equality.","Compares the lower element of <code>a</code> and <code>b</code> for ","Compares the lower element of <code>a</code> and <code>b</code> for greater-than.","Compares the lower element of <code>a</code> and <code>b</code> for ","Compares the lower element of <code>a</code> and <code>b</code> for less-than.","Compares the lower element of <code>a</code> and <code>b</code> for not-equal.","Converts the lower two packed 32-bit integers in <code>a</code> to ","Converts packed 32-bit integers in <code>a</code> to packed ","Converts packed double-precision (64-bit) floating-point ","Converts packed double-precision (64-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Returns the lower double-precision (64-bit) ","Converts the lower double-precision (64-bit) ","Converts the lower double-precision (64-bit) ","Returns the lowest element of <code>a</code>.","Returns the lowest element of <code>a</code>.","Returns the lowest element of <code>a</code>.","Returns <code>a</code> with its lower element replaced by <code>b</code> after ","Returns a vector whose lowest element is <code>a</code> and all higher ","Returns a vector whose lowest element is <code>a</code> and all higher ","Converts the lower single-precision (32-bit) ","Converts packed double-precision (64-bit) floating-point ","Converts packed single-precision (32-bit) floating-point ","Converts the lower double-precision (64-bit) ","Converts the lower double-precision (64-bit) ","Alias for <code>_mm_cvttsd_si64</code>","Divide packed double-precision (64-bit) floating-point ","Returns a new vector with the low element of <code>a</code> replaced ","Returns the <code>imm8</code> element of <code>a</code>.","Returns a new vector where the <code>imm8</code> element of <code>a</code> is ","Performs a serializing operation on all load-from-memory ","Loads a double-precision (64-bit) floating-point element ","Loads 128-bits (composed of 2 packed double-precision ","Loads a double-precision (64-bit) floating-point element ","Loads a 64-bit double-precision value to the low element ","Loads 128-bits of integer data from memory into a new ","Loads a double-precision value into the high-order bits ","Loads 64-bit integer from memory into first element of ","Loads a double-precision value into the low-order bits of ","Loads 2 double-precision (64-bit) floating-point elements ","Loads 128-bits (composed of 2 packed double-precision ","Loads 128-bits of integer data from memory into a new ","Multiplies and then horizontally add signed 16 bit ","Conditionally store 8-bit integer elements from <code>a</code> into ","Compares packed 16-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed unsigned 8-bit integers in <code>a</code> and <code>b</code>, and ","Returns a new vector with the maximum values from ","Returns a new vector with the low element of <code>a</code> replaced ","Performs a serializing operation on all load-from-memory ","Compares packed 16-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed unsigned 8-bit integers in <code>a</code> and <code>b</code>, and ","Returns a new vector with the minimum values from ","Returns a new vector with the low element of <code>a</code> replaced ","Returns a vector where the low element is extracted from <code>a</code>","Constructs a 128-bit floating-point vector of <code>[2 x double]</code>","Returns a mask of the most significant bit of each ","Returns a mask of the most significant bit of each ","Multiplies the low unsigned 32-bit integers from each ","Multiplies packed double-precision (64-bit) ","Returns a new vector with the low element of <code>a</code> replaced ","Multiplies the packed 16-bit integers in <code>a</code> and <code>b</code>.","Multiplies the packed unsigned 16-bit integers in <code>a</code> and <code>b</code>.","Multiplies the packed 16-bit integers in <code>a</code> and <code>b</code>.","Computes the bitwise OR of <code>a</code> and <code>b</code>.","Computes the bitwise OR of 128 bits (representing integer ","Converts packed 16-bit integers from <code>a</code> and <code>b</code> to packed ","Converts packed 32-bit integers from <code>a</code> and <code>b</code> to packed ","Converts packed 16-bit integers from <code>a</code> and <code>b</code> to packed ","Provides a hint to the processor that the code sequence ","Sum the absolute differences of packed unsigned 8-bit ","Broadcasts 16-bit integer <code>a</code> to all elements.","Broadcasts 32-bit integer <code>a</code> to all elements.","Broadcasts 64-bit integer <code>a</code> to all elements.","Broadcasts 8-bit integer <code>a</code> to all elements.","Broadcasts double-precision (64-bit) floating-point value ","Sets packed 16-bit integers with the supplied values.","Sets packed 32-bit integers with the supplied values.","Sets packed 64-bit integers with the supplied values, ","Sets packed 8-bit integers with the supplied values.","Sets packed double-precision (64-bit) floating-point ","Broadcasts double-precision (64-bit) floating-point value ","Copies double-precision (64-bit) floating-point element <code>a</code> ","Sets packed 16-bit integers with the supplied values in ","Sets packed 32-bit integers with the supplied values in ","Sets packed 8-bit integers with the supplied values in ","Sets packed double-precision (64-bit) floating-point ","Returns packed double-precision (64-bit) floating-point ","Returns a vector with all elements set to zero.","Shuffles 32-bit integers in <code>a</code> using the control in <code>imm8</code>.","Constructs a 128-bit floating-point vector of <code>[2 x double]</code>","Shuffles 16-bit integers in the high 64 bits of <code>a</code> using ","Shuffles 16-bit integers in the low 64 bits of <code>a</code> using ","Shifts packed 16-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 64-bit integers in <code>a</code> left by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts packed 64-bit integers in <code>a</code> left by <code>imm8</code> while ","Shifts <code>a</code> left by <code>imm8</code> bytes while shifting in zeros.","Returns a new vector with the square root of each of the ","Returns a new vector with the low element of <code>a</code> replaced ","Shifts packed 16-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 16-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 64-bit integers in <code>a</code> right by <code>count</code> while ","Shifts packed 16-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 32-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts packed 64-bit integers in <code>a</code> right by <code>imm8</code> while ","Shifts <code>a</code> right by <code>imm8</code> bytes while shifting in zeros.","Stores the lower double-precision (64-bit) floating-point ","Stores 128-bits (composed of 2 packed double-precision ","Stores the lower double-precision (64-bit) floating-point ","Stores the lower 64 bits of a 128-bit vector of ","Stores 128-bits of integer data from <code>a</code> into memory.","Stores the upper 64 bits of a 128-bit vector of ","Stores the lower 64-bit integer <code>a</code> to a memory location.","Stores the lower 64 bits of a 128-bit vector of ","Stores 2 double-precision (64-bit) floating-point ","Stores 128-bits (composed of 2 packed double-precision ","Stores 128-bits of integer data from <code>a</code> into memory.","Stores a 128-bit floating point vector of <code>[2 x double]</code> to ","Stores a 128-bit integer vector to a 128-bit aligned ","Stores a 32-bit integer value in the specified memory ","Stores a 64-bit integer value in the specified memory ","Subtracts packed 16-bit integers in <code>b</code> from packed 16-bit ","Subtract packed 32-bit integers in <code>b</code> from packed 32-bit ","Subtract packed 64-bit integers in <code>b</code> from packed 64-bit ","Subtracts packed 8-bit integers in <code>b</code> from packed 8-bit ","Subtract packed double-precision (64-bit) floating-point ","Returns a new vector with the low element of <code>a</code> replaced ","Subtract packed 16-bit integers in <code>b</code> from packed 16-bit ","Subtract packed 8-bit integers in <code>b</code> from packed 8-bit ","Subtract packed unsigned 16-bit integers in <code>b</code> from packed ","Subtract packed unsigned 8-bit integers in <code>b</code> from packed ","Compares the lower element of <code>a</code> and <code>b</code> for equality.","Compares the lower element of <code>a</code> and <code>b</code> for ","Compares the lower element of <code>a</code> and <code>b</code> for greater-than.","Compares the lower element of <code>a</code> and <code>b</code> for ","Compares the lower element of <code>a</code> and <code>b</code> for less-than.","Compares the lower element of <code>a</code> and <code>b</code> for not-equal.","Returns vector of type __m128d with undefined elements.","Returns vector of type __m128i with undefined elements.","Unpacks and interleave 16-bit integers from the high half ","Unpacks and interleave 32-bit integers from the high half ","Unpacks and interleave 64-bit integers from the high half ","Unpacks and interleave 8-bit integers from the high half ","The resulting <code>__m128d</code> element is composed by the ","Unpacks and interleave 16-bit integers from the low half ","Unpacks and interleave 32-bit integers from the low half ","Unpacks and interleave 64-bit integers from the low half ","Unpacks and interleave 8-bit integers from the low half ","The resulting <code>__m128d</code> element is composed by the ","Computes the bitwise OR of <code>a</code> and <code>b</code>.","Computes the bitwise XOR of 128 bits (representing ","Alternatively add and subtract packed double-precision ","Alternatively add and subtract packed single-precision ","Horizontally adds adjacent pairs of double-precision ","Horizontally adds adjacent pairs of single-precision ","Horizontally subtract adjacent pairs of double-precision ","Horizontally adds adjacent pairs of single-precision ","Loads 128-bits of integer data from unaligned memory. ","Loads a double-precision (64-bit) floating-point element ","Duplicate the low double-precision (64-bit) ","Duplicate odd-indexed single-precision (32-bit) ","Duplicate even-indexed single-precision (32-bit) ","Blend packed 16-bit integers from <code>a</code> and <code>b</code> using the mask ","Blend packed double-precision (64-bit) floating-point ","Blend packed single-precision (32-bit) floating-point ","Blend packed 8-bit integers from <code>a</code> and <code>b</code> using <code>mask</code>","Blend packed double-precision (64-bit) floating-point ","Blend packed single-precision (32-bit) floating-point ","Round the packed double-precision (64-bit) floating-point ","Round the packed single-precision (32-bit) floating-point ","Round the lower double-precision (64-bit) floating-point ","Round the lower single-precision (32-bit) floating-point ","Compares packed 64-bit integers in <code>a</code> and <code>b</code> for equality","Sign extend packed 16-bit integers in <code>a</code> to packed 32-bit ","Sign extend packed 16-bit integers in <code>a</code> to packed 64-bit ","Sign extend packed 32-bit integers in <code>a</code> to packed 64-bit ","Sign extend packed 8-bit integers in <code>a</code> to packed 16-bit ","Sign extend packed 8-bit integers in <code>a</code> to packed 32-bit ","Sign extend packed 8-bit integers in the low 8 bytes of <code>a</code> ","Zeroes extend packed unsigned 16-bit integers in <code>a</code> to ","Zeroes extend packed unsigned 16-bit integers in <code>a</code> to ","Zeroes extend packed unsigned 32-bit integers in <code>a</code> to ","Zeroes extend packed unsigned 8-bit integers in <code>a</code> to ","Zeroes extend packed unsigned 8-bit integers in <code>a</code> to ","Zeroes extend packed unsigned 8-bit integers in <code>a</code> to ","Returns the dot product of two __m128d vectors.","Returns the dot product of two __m128 vectors.","Extracts an 32-bit integer from <code>a</code> selected with <code>imm8</code>","Extracts an 64-bit integer from <code>a</code> selected with <code>imm8</code>","Extracts an 8-bit integer from <code>a</code>, selected with <code>imm8</code>. ","Extracts a single-precision (32-bit) floating-point ","Round the packed double-precision (64-bit) floating-point ","Round the packed single-precision (32-bit) floating-point ","Round the lower double-precision (64-bit) floating-point ","Round the lower single-precision (32-bit) floating-point ","Returns a copy of <code>a</code> with the 32-bit integer from <code>i</code> ","Returns a copy of <code>a</code> with the 64-bit integer from <code>i</code> ","Returns a copy of <code>a</code> with the 8-bit integer from <code>i</code> ","Select a single value in <code>a</code> to store at some position in <code>b</code>,","Compares packed 32-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 8-bit integers in <code>a</code> and <code>b</code> and returns ","Compares packed unsigned 16-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 32-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed 32-bit integers in <code>a</code> and <code>b</code>, and returns ","Compares packed 8-bit integers in <code>a</code> and <code>b</code> and returns ","Compares packed unsigned 16-bit integers in <code>a</code> and <code>b</code>, and ","Compares packed unsigned 32-bit integers in <code>a</code> and <code>b</code>, and ","Finds the minimum unsigned 16-bit element in the 128-bit _","Subtracts 8-bit unsigned integer values and computes the ","Multiplies the low 32-bit integers from each packed 64-bit","Multiplies the packed 32-bit integers in <code>a</code> and <code>b</code>, ","Converts packed 32-bit integers from <code>a</code> and <code>b</code> to packed ","Round the packed double-precision (64-bit) floating-point ","Round the packed single-precision (32-bit) floating-point ","Round the lower double-precision (64-bit) floating-point ","Round the lower single-precision (32-bit) floating-point ","Tests whether the specified bits in <code>a</code> 128-bit integer ","Tests whether the specified bits in a 128-bit integer ","Tests whether the specified bits in a 128-bit integer ","Tests whether the specified bits in a 128-bit integer ","Tests whether the specified bits in a 128-bit integer ","Tests whether the specified bits in a 128-bit integer ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed strings in <code>a</code> and <code>b</code> with lengths <code>la</code> and <code>lb</code> ","Compares packed 64-bit integers in <code>a</code> and <code>b</code> for ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Compares packed strings with implicit lengths in <code>a</code> and <code>b</code> ","Starting with the initial value in <code>crc</code>, return the ","Starting with the initial value in <code>crc</code>, return the ","Starting with the initial value in <code>crc</code>, return the ","Starting with the initial value in <code>crc</code>, return the ","Computes the absolute value of each of the packed 16-bit ","Computes the absolute value of each of the packed 32-bit ","Computes the absolute value of packed 8-bit signed ","Concatenate 16-byte blocks in <code>a</code> and <code>b</code> into a 32-byte ","Horizontally adds the adjacent pairs of values contained ","Horizontally adds the adjacent pairs of values contained ","Horizontally adds the adjacent pairs of values contained ","Horizontally subtract the adjacent pairs of values ","Horizontally subtract the adjacent pairs of values ","Horizontally subtract the adjacent pairs of values ","Multiplies corresponding pairs of packed 8-bit unsigned ","Multiplies packed 16-bit signed integer values, truncate ","Shuffles bytes from <code>a</code> according to the content of <code>b</code>.","Negates packed 16-bit integers in <code>a</code> when the ","Negates packed 32-bit integers in <code>a</code> when the ","Negates packed 8-bit integers in <code>a</code> when the corresponding ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Umbrella trait for SIMD vector element bounds","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Returns the remainder of the slice that is being iterated ","Returns the remainder of the slice that is being iterated ","","","","","","","","","","","","","","","","","","","","Precision is important, but not the focus, so avoid ","Precision is very important, so do everything to improve ","","Eulers number (e)","1/","1/sqrt(2)","1/sqrt()","2/","2/sqrt()","/2","/3","/4","/6","/8","ln(10)","ln(2)","ln()","log10(2)","log10(e)","log2(10)","log2(e)","Archimedes constant ()","The specific policy used. This is a constant to allow for ","Execution policy used for controlling ","Customizable Policy Parameters","Precision Policy, tradeoffs between precision and ","Precision is the only factor, use infinite sums to ","sqrt(2)","sqrt(e)","sqrt(/2)","","Set of vectorized special functions optimized for both ","Set of vectorized special functions allowing specific ","The full circle constant ()","Precision is not important, so prefer simpler or faster ","Computes the arccosine of a vector.","","Computes the hyperbolic-arccosine of a vector.","","Computes the arcsine of a vector.","","Computes the hyperbolic-arcsine of a vector.","","Computes the arctangent of a vector.","Computes the four quadrant arc-tangent of <code>y</code>(<code>self</code>) and <code>x</code>","","","Computes the hyperbolic-arctangent of a vector.","","If true, methods will not try to avoid extra work by ","Returns true if the policy says to avoid branches at the ","","","","","","","","","Computes the cubic-root of each lane in a vector.","","If true, methods will check for infinity/NaN/invalid ","","","","","Computes the cosine of a vector.","","Computes the hyperbolic-cosine of a vector.","","","Computes the error function for each value in a vector.","","Computes the inverse error function for each value in a ","","The exponential function, returns <code>e^(self)</code>","Base-10 exponential function, returns <code>10^(self)</code>","","Binary exponential function, returns <code>2^(self)</code>","","Exponential function minus one, <code>e^(self) - 1.0</code>, special ","","","Half-exponential function, returns <code>0.5 * e^(self)</code>","","Returns the floating-point remainder of <code>self / y</code> (rounded ","","","","","","","","","","Computes <code>sqrt(x * x + y * y)</code> for each element of the ","","","","Computes the approximate inverse square root (<code>1/sqrt(x)</code>)","","Linearly interpolates between <code>a</code> and <code>b</code> using <code>self</code>","","Computes the natural logarithm of a vector.","Computes <code>ln(1+x)</code> where <code>x</code> is <code>self</code>, more accurately than if ","","","Computes the base-10 logarithm of a vector","","Computes the base-2 logarithm of a vector","","Some special functions require many, many iterations of a ","Finds the next representable float moving upwards to ","","","Execution Policies (precision, performance, etc.)","Optimized fixed-degree polynomial evaluation","Computes the sum <code>(coefficients[i] * x^i)</code> from <code>i=0</code> to ","Computes the sum <code>(f(i)*x^i)</code> from <code>i=0</code> to <code>n</code>","","","Computes ","","Computes <code>x^e</code> where <code>x</code> is <code>self</code> and <code>e</code> is a vector of ","","Computes <code>x^e</code> where <code>x</code> is <code>self</code> and <code>e</code> is a signed integer","","Computes <code>x^e</code> where <code>x</code> is <code>self</code> and <code>e</code> is a vector of integer ","","Controls if precision should be emphasized or ","Finds the previous representable float moving downwards ","","Computes <code>[f(n)]</code> from <code>n=start</code> to <code>end</code>, <strong>terminating early ","","Computes the approximate reciprocal <code>1/x</code> variation, which ","","Scales values between <code>in_min</code> and <code>in_max</code>, to between ","","Computes the sine of a vector.","Computes both the sine and cosine of a vector together ","","","Computes the hyperbolic-sine of a vector.","","Calculates a sigmoid-like 5th-order interpolation function","","Calculates a sigmoid-like 7th-order interpolation function","","Calculates a sigmoid-like 3rd-order interpolation function","","Computes <code>[f(n)]</code> from <code>n=0</code> to <code>end</code>.","","Computes <code>[f(n)]</code> from <code>n=start</code> to <code>end</code>, <strong>terminating early ","","Computes the tangent of a vector.","","Computes the hyperbolic-tangent of a vector.","","","","","","","","","If true, unrolled and optimized versions of some ","","","","","","","","","","","","","","","","","","","","","","","","","Policy adapter that increases the precision requires by ","Optimize for performance, ideally without losing ","Optimize for precision, at the cost of performance if ","Calculates a reference value for operations where ","Optimize for code size, avoids hard-coded equations or ","Optimize for performance at the cost of precision and ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"i":[1,2,2,0,0,0,0,1,1,3,4,5,6,0,1,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,0,0,0,6,0,6,8,0,6,0,6,0,6,8,0,6,0,6,0,6,9,10,11,4,12,1,3,4,11,4,8,0,12,12,8,12,12,0,11,4,11,4,3,4,11,4,11,4,13,4,14,4,11,4,11,4,12,15,11,16,10,4,2,12,15,11,16,10,4,2,17,12,15,11,16,10,4,2,17,12,15,11,16,10,4,2,18,4,8,8,15,11,16,10,4,2,15,11,16,10,4,2,2,8,19,9,19,9,11,4,20,20,4,12,16,12,0,0,12,0,8,19,15,11,16,10,2,1,1,4,11,4,12,8,12,11,10,4,2,8,12,15,11,16,10,4,2,4,21,22,12,15,11,16,10,4,2,22,12,15,11,16,10,4,2,4,12,19,19,1,1,19,19,2,2,2,16,16,16,16,19,8,12,15,11,16,10,4,2,23,8,8,8,9,8,10,9,24,8,8,0,12,12,19,20,20,12,12,1,1,4,8,8,1,1,4,12,24,19,1,0,19,19,19,19,19,9,19,8,8,8,8,15,8,11,19,16,10,8,9,8,24,8,8,8,8,11,4,11,4,2,19,2,8,11,8,10,10,1,1,4,1,20,0,20,20,20,20,8,8,2,8,20,20,19,19,1,1,4,9,15,1,0,1,1,1,4,9,1,10,4,1,1,8,1,1,4,8,8,1,1,4,12,8,4,8,15,11,16,10,4,2,8,20,20,8,11,4,12,15,11,16,10,4,2,12,15,11,16,10,4,2,12,15,11,16,10,4,2,15,16,15,15,16,15,15,16,15,15,16,15,1,4,20,20,10,10,19,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,25,26,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,28,29,30,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,0,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,34,35,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,34,34,35,35,35,36,36,36,37,37,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,32,32,33,33,34,34,35,35,36,36,37,37,38,32,33,34,35,36,37,0,0,32,33,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,34,35,32,32,32,32,32,32,33,33,33,33,33,33,34,34,34,34,34,34,35,35,35,35,35,35,36,36,36,36,36,36,37,37,37,37,37,37,38,32,32,32,32,32,32,33,33,33,33,33,33,34,34,34,34,34,34,35,35,35,35,35,35,36,36,36,36,36,36,37,37,37,37,37,37,38,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,35,36,37,38,0,0,32,33,34,35,36,37,32,33,32,33,34,35,36,37,38,32,33,34,35,34,35,34,35,32,33,32,33,32,33,34,35,36,37,32,33,34,35,36,37,32,33,32,33,34,36,32,33,32,33,34,36,32,33,34,35,36,37,32,33,34,35,36,37,32,33,34,36,32,33,34,35,36,37,32,33,34,35,32,33,34,35,36,37,32,33,34,35,36,37,32,33,32,33,32,32,33,33,34,34,35,35,36,36,37,37,32,33,32,33,32,33,32,32,33,33,32,33,34,35,32,33,32,33,34,35,32,33,36,37,32,33,32,33,32,33,32,33,32,33,34,35,36,37,32,33,34,35,36,37,32,33,32,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,34,35,36,37,34,35,36,37,34,35,36,37,32,33,32,34,35,36,37,34,35,36,37,32,33,34,35,32,32,33,33,34,34,35,35,36,36,37,37,32,32,33,33,34,34,35,35,36,36,37,37,32,32,33,33,34,34,35,35,36,36,37,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,34,35,36,37,32,33,32,33,34,35,36,37,32,33,32,33,34,35,36,37,32,33,32,33,34,36,32,33,34,35,36,37,32,32,33,33,34,34,35,35,36,36,37,37,32,33,32,33,32,33,34,35,36,37,38,32,33,34,35,36,37,32,33,32,33,34,35,36,37,38,32,33,34,35,36,37,38,32,33,34,35,36,37,38,0,0,32,33,34,35,36,37,34,35,36,37,34,35,36,37,32,33,34,35,36,37,0,0,0,0,39,0,0,40,0,40,0,0,0,41,42,43,44,41,42,43,44,41,42,43,45,44,41,42,43,44,43,44,43,44,41,42,43,44,41,42,43,44,41,42,43,44,41,42,43,44,42,43,44,40,41,41,43,41,42,43,44,42,43,42,43,45,43,44,43,44,41,42,43,44,41,42,43,44,41,42,43,44,46,46,0,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,47,48,0,0,0,46,47,47,47,0,0,0,47,46,49,50,49,50,49,50,49,50,49,49,50,50,49,50,51,51,51,46,51,46,51,46,51,46,49,50,51,46,46,46,0,49,50,49,50,46,49,50,49,50,49,49,50,49,50,49,50,50,49,50,49,50,46,51,46,51,46,51,46,46,49,50,51,46,49,50,49,50,49,49,50,50,49,50,49,50,51,49,50,46,0,0,49,49,50,50,49,50,49,50,49,50,49,50,51,49,50,49,50,49,50,49,50,49,49,50,50,49,50,49,50,49,50,49,50,49,50,49,50,49,50,49,50,46,51,46,51,46,51,46,51,0,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,0,0,0,0,0,0,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,53,54,55,56,57,58,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,59,59,59,0,59,0,0,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,60,0,0,0,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63,61,62,63],"f":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[]],[[]],[[],["bool",15]],[[],["bool",15]],[[["usize",15]]],[[["usize",15]],["vectorbuffer",3]],[[]],[[]],[[],["bool",15]],[[],["bool",15]],[[],["mask",3]],null,[[]],[[]],[[]],[[]],[[]],null,[[]],[[]],[[]],[[]],[[],["u16",15]],[[],["bitmask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[],["simdfromcast",8]],[[],[["mask",3],["simdfromcast",8]]],[[]],[[]],[[]],[[]],[[]],[[],["vptr",3]],[[],["mask",3]],[[],["simdinstructionset",4]],[[]],[[]],[[]],[[]],[[]],[[]],[[["simdinstructionset",4]],["ordering",4]],[[]],[[["mask",3]]],[[["mask",3]]],[[["mask",3]]],[[]],[[],["u32",15]],[[],["u32",15]],[[]],[[]],[[]],[[]],[[]],[[]],null,null,[[]],null,[[]],[[],["mask",3]],[[],["bool",15]],[[],["bool",15]],[[["branchfreedivider",3]],["bool",15]],[[["vptr",3]],["bool",15]],[[["simdinstructionset",4]],["bool",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]],["bool",15]],[[]],[[]],[[]],[[]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["u16",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[["mask",3]]],[[["mask",3]]],[[]],[[],["mask",3]],[[],["mask",3]],[[],["bool",15]],[[],["bool",15]],[[]],[[["i16",15]]],[[["i32",15]]],[[["i64",15]]],[[["i8",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],[[],["mask",3]],null,[[],["simdsliceiter",3]],[[],["alignedmutiter",3]],[[],["mask",3]],[[]],[[]],[[],["usize",15]],[[],["usize",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["usize",15]]],[[]],[[],["mask",3]],[[]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["bool",15]],[[],["mask",3]],[[["branchfreedivider",3]],["bool",15]],[[["vptr",3]],["bool",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["bool",15]],[[],["bool",15]],[[]],[[]],[[],["usize",15]],[[]],[[["simdinstructionset",4]],[["option",4],["ordering",4]]],[[]],[[],["u16",15]],[[]],[[],["associatedvector",6]],[[["mask",3],["associatedvector",6]],["associatedvector",6]],[[["usize",15]]],[[["usize",15]]],[[["usize",15],["bool",15]]],[[]],[[]],null,[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[]],[[]],[[],["simdinstructionset",4]],[[]],[[]],[[]],[[]],[[["mask",3]]],[[["mask",3]]],[[]],[[]],[[]],[[],["u8",15]],[[["simdshuffleindices",8]]],null,[[]],[[]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[]],[[]],[[]],[[["bool",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[["u16",15]]],[[["u16",15]]],[[["u16",15]],[["branchfreedivider",3],["u16",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]],[["u32",15],["branchfreedivider",3]]],[[["u64",15]]],[[["u64",15]]],[[["u64",15]],[["u64",15],["branchfreedivider",3]]],[[["u8",15]]],[[["u8",15]]],[[["u8",15]],[["u8",15],["branchfreedivider",3]]],[[]],[[]],[[]],[[]],[[["associatedvector",6]]],[[["mask",3],["associatedvector",6]]],[[]],null,null,null,null,null,null,null,null,null,null,null,null,null,[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m128d",3]],["__m256d",3]],[[["__m128",3]],["__m256",3]],[[["f64",15]],["__m256d",3]],[[["f32",15]],["__m256",3]],[[["__m128d",3]],["__m256d",3]],[[["__m256d",3]],["__m128d",3]],[[["__m256d",3]],["__m256",3]],[[["__m256d",3]],["__m256i",3]],[[["__m128",3]],["__m256",3]],[[["__m256",3]],["__m128",3]],[[["__m256",3]],["__m256d",3]],[[["__m256",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m256i",3]],["__m256d",3]],[[["__m256i",3]],["__m256",3]],[[["__m256i",3]],["__m128i",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m128i",3]],["__m256d",3]],[[["__m256i",3]],["__m256",3]],[[["__m256d",3]],["__m128i",3]],[[["__m256d",3]],["__m128",3]],[[["__m256",3]],["__m256i",3]],[[["__m128",3]],["__m256d",3]],[[["__m256d",3]],["f64",15]],[[["__m256i",3]],["i32",15]],[[["__m256",3]],["f32",15]],[[["__m256d",3]],["__m128i",3]],[[["__m256",3]],["__m256i",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256i",3],["i32",15]],["i32",15]],[[["__m256i",3],["i32",15]],["i64",15]],[[["__m256d",3],["i32",15]],["__m128d",3]],[[["__m256",3],["i32",15]],["__m128",3]],[[["__m256i",3],["i32",15]],["__m128i",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256i",3],["i16",15],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i64",15],["i32",15]],["__m256i",3]],[[["__m256i",3],["i8",15],["i32",15]],["__m256i",3]],[[["__m256d",3],["i32",15],["__m128d",3]],["__m256d",3]],[[["__m256",3],["__m128",3],["i32",15]],["__m256",3]],[[["__m256i",3],["i32",15],["__m128i",3]],["__m256i",3]],[[],["__m256i",3]],[[],["__m256d",3]],[[],["__m256",3]],[[],["__m256i",3]],[[],["__m256",3]],[[],["__m256d",3]],[[],["__m256i",3]],[[],["__m256d",3]],[[],["__m256",3]],[[],["__m256i",3]],[[["__m256i",3]],["__m256d",3]],[[["__m256i",3]],["__m256",3]],[[["__m256i",3],["__m256d",3]]],[[["__m256i",3],["__m256",3]]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["i32",15]],[[["__m256",3]],["i32",15]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256i",3],["__m256d",3]],["__m256d",3]],[[["__m256i",3],["__m256",3]],["__m256",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256",3]],["__m256",3]],[[["i16",15]],["__m256i",3]],[[["i32",15]],["__m256i",3]],[[["i64",15]],["__m256i",3]],[[["i8",15]],["__m256i",3]],[[["f64",15]],["__m256d",3]],[[["f32",15]],["__m256",3]],[[["i16",15]],["__m256i",3]],[[["i32",15]],["__m256i",3]],[[["i64",15]],["__m256i",3]],[[["i8",15]],["__m256i",3]],[[["__m128",3]],["__m256",3]],[[["__m128d",3]],["__m256d",3]],[[["__m128i",3]],["__m256i",3]],[[["f64",15]],["__m256d",3]],[[["f32",15]],["__m256",3]],[[["i16",15]],["__m256i",3]],[[["i32",15]],["__m256i",3]],[[["i64",15]],["__m256i",3]],[[["i8",15]],["__m256i",3]],[[["__m128",3]],["__m256",3]],[[["__m128d",3]],["__m256d",3]],[[["__m128i",3]],["__m256i",3]],[[["f64",15]],["__m256d",3]],[[["f32",15]],["__m256",3]],[[],["__m256d",3]],[[],["__m256",3]],[[],["__m256i",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256",3],["i32",15]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]]],[[["__m256",3]]],[[["__m256i",3]]],[[["__m256",3]]],[[["__m256d",3]]],[[["__m256i",3]]],[[["__m256d",3]]],[[["__m256",3]]],[[["__m256i",3]]],[[["__m256d",3]]],[[["__m256",3]]],[[["__m256i",3]]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["i32",15]],[[["__m256",3]],["i32",15]],[[["__m256i",3]],["i32",15]],[[["__m256d",3]],["i32",15]],[[["__m256",3]],["i32",15]],[[["__m256i",3]],["i32",15]],[[["__m256d",3]],["i32",15]],[[["__m256",3]],["i32",15]],[[["__m256i",3]],["i32",15]],[[],["__m256d",3]],[[],["__m256",3]],[[],["__m256i",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[]],[[]],[[["__m128d",3]],["__m256d",3]],[[["__m128",3]],["__m256",3]],[[["__m128i",3]],["__m256i",3]],[[["f32",15]],["__m128",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128i",3]],["__m128d",3]],[[["__m128i",3]],["__m128",3]],[[["__m128d",3],["__m128i",3]]],[[["__m128",3],["__m128i",3]]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128i",3],["__m128d",3]],["__m128d",3]],[[["__m128",3],["__m128i",3]],["__m128",3]],[[["__m128d",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[],["__m256i",3]],[[],["__m256",3]],[[],["__m256d",3]],[[]],[[]],[[]],[[["formatter",3]],[["result",4],["error",3]]],[[["formatter",3]],[["result",4],["error",3]]],[[["formatter",3]],[["result",4],["error",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128d",3]],["__m256d",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128",3]],["__m256",3]],[[["__m128i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m128i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["i32",15]],[[["__m256i",3],["i32",15]],["i32",15]],[[["__m256i",3],["i32",15]],["__m128i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m128i",3],["i32",15]],["__m256i",3]],[[["__m128i",3],["i32",15]],["__m256d",3]],[[["__m256i",3],["i32",15]],["__m256",3]],[[["__m256i",3],["i32",15]],["__m128i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256d",3]],[[["__m256i",3],["i32",15]],["__m128",3]],[[["__m256i",3],["i32",15],["__m128i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15],["__m128i",3]],["__m256i",3]],[[["__m256d",3],["i32",15],["__m128i",3]],["__m256d",3]],[[["__m256i",3],["__m256",3],["i32",15]],["__m256",3]],[[["__m256i",3],["i32",15],["__m128i",3]],["__m128i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["__m256d",3],["i32",15]],["__m256d",3]],[[["__m128",3],["__m256i",3],["i32",15]],["__m128",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]]],[[["__m256i",3]]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["i32",15]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256d",3],["i32",15]],["__m256d",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["__m256",3]],["__m256",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["__m128i",3]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3],["i32",15]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["__m256i",3]],["__m256i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3],["i32",15]],["__m128i",3]],[[["__m128i",3],["i32",15]],["__m128d",3]],[[["i32",15],["__m128i",3]],["__m128",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3],["i32",15]],["__m128i",3]],[[["__m128i",3],["i32",15]],["__m128d",3]],[[["i32",15],["__m128i",3]],["__m128",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3],["__m128d",3],["i32",15]],["__m128d",3]],[[["__m128",3],["i32",15],["__m128i",3]],["__m128",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3],["__m128d",3],["i32",15]],["__m128d",3]],[[["__m128",3],["i32",15],["__m128i",3]],["__m128",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]]],[[["__m128i",3]]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m256d",3]],["__m256d",3]],[[["__m256",3]],["__m256",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128",3]],["i32",15]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128",3],["i64",15]],["__m128",3]],[[["__m128",3]],["f32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i64",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i64",15]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[],["__m128",3]],[[],["__m128",3]],[[],["__m128",3]],[[],["__m128",3]],[[],["__m128",3]],[[],["__m128",3]],[[],["__m128i",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["i32",15]]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["f32",15]],["__m128",3]],[[["f32",15]],["__m128",3]],[[["f32",15]],["__m128",3]],[[["f32",15]],["__m128",3]],[[["u32",15]]],[[["f32",15]],["__m128",3]],[[],["__m128",3]],[[]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[["__m128",3]],["i32",15]],[[],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[],["__m128i",3]],[[],["__m128",3]],[[],["__m128d",3]],[[]],[[]],[[]],[[["formatter",3]],[["result",4],["error",3]]],[[["formatter",3]],[["result",4],["error",3]]],[[["formatter",3]],[["result",4],["error",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128",3]],[[["__m128d",3]],["__m128i",3]],[[["__m128",3]],["__m128d",3]],[[["__m128",3]],["__m128i",3]],[[["__m128i",3]],["__m128d",3]],[[["__m128i",3]],["__m128",3]],[[]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128i",3]],["__m128d",3]],[[["__m128i",3]],["__m128",3]],[[["__m128d",3]],["__m128i",3]],[[["__m128d",3]],["__m128",3]],[[["__m128",3]],["__m128i",3]],[[["__m128",3]],["__m128d",3]],[[["__m128d",3]],["f64",15]],[[["__m128d",3]],["i32",15]],[[["__m128",3],["__m128d",3]],["__m128",3]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i64",15]],[[["__m128i",3]],["i64",15]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["i32",15]],["__m128i",3]],[[["i64",15]],["__m128i",3]],[[["__m128",3],["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128i",3]],[[["__m128",3]],["__m128i",3]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i64",15]],[[["__m128d",3]],["i64",15]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[]],[[],["__m128d",3]],[[],["__m128d",3]],[[],["__m128d",3]],[[],["__m128d",3]],[[],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[],["__m128d",3]],[[],["__m128d",3]],[[],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[]],[[["__m128i",3]],["__m128i",3]],[[["i16",15]],["__m128i",3]],[[["i32",15]],["__m128i",3]],[[["i64",15]],["__m128i",3]],[[["i8",15]],["__m128i",3]],[[["f64",15]],["__m128d",3]],[[["i16",15]],["__m128i",3]],[[["i32",15]],["__m128i",3]],[[["i64",15]],["__m128i",3]],[[["i8",15]],["__m128i",3]],[[["f64",15]],["__m128d",3]],[[["f64",15]],["__m128d",3]],[[["f64",15]],["__m128d",3]],[[["i16",15]],["__m128i",3]],[[["i32",15]],["__m128i",3]],[[["i8",15]],["__m128i",3]],[[["f64",15]],["__m128d",3]],[[],["__m128d",3]],[[],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128d",3]]],[[["__m128d",3]]],[[["__m128d",3]]],[[["__m128d",3]]],[[["__m128i",3]]],[[["__m128d",3]]],[[["__m128i",3]]],[[["__m128d",3]]],[[["__m128d",3]]],[[["__m128d",3]]],[[["__m128i",3]]],[[["__m128d",3]]],[[["__m128i",3]]],[[["i32",15]]],[[["i64",15]]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[["__m128d",3]],["i32",15]],[[],["__m128d",3]],[[],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[],["__m128i",3]],[[],["__m128d",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128",3]],["__m128",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i64",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["__m128",3],["i32",15]],["i32",15]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["__m128d",3]],["__m128d",3]],[[["__m128",3]],["__m128",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i64",15],["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["i32",15],["__m128d",3]],["__m128d",3]],[[["__m128",3],["i32",15]],["__m128",3]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i32",15]],[[["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["i32",15],["__m128i",3]],["i32",15]],[[["u32",15],["u16",15]],["u32",15]],[[["u32",15]],["u32",15]],[[["u64",15]],["u64",15]],[[["u32",15],["u8",15]],["u32",15]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["i32",15],["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],[[["__m128i",3]],["__m128i",3]],null,null,null,null,null,null,[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[],["avx1",3]],[[]],[[["avx1",3]],["bool",15]],[[["formatter",3]],["result",6]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["u16",15]],[[],["u16",15]],[[],["u16",15]],[[],["u16",15]],[[],["u16",15]],[[],["u16",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[],["f32x8",3]],[[],["f64x8",3]],[[],["i32x8",3]],[[],["i64x8",3]],[[],["u32x8",3]],[[],["u64x8",3]],[[],["avx2",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["i32",15],["divider",3]]],[[["i32",15],["branchfreedivider",3]]],[[]],[[["divider",3],["i64",15]]],[[]],[[["branchfreedivider",3],["i64",15]]],[[["u32",15],["branchfreedivider",3]]],[[]],[[["u32",15],["divider",3]]],[[]],[[["u64",15],["branchfreedivider",3]]],[[["divider",3],["u64",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[],["bool",15]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[["avx2",3]],["bool",15]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],null,null,[[]],[[]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["u64x8",3]]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["u64x8",3]]],[[["avx2",3],["u64x8",3]]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["i32x8",3]]],[[]],[[["avx2",3],["i64x8",3]]],[[["avx2",3],["f64x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["f32x8",3]]],[[["u32x8",3],["avx2",3]]],[[]],[[["avx2",3],["i64x8",3]]],[[["avx2",3],["u64x8",3]]],[[]],[[["avx2",3],["i64x8",3]]],[[["avx2",3],["u64x8",3]]],[[["avx2",3],["f32x8",3]]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["f64x8",3]]],[[["avx2",3],["u64x8",3]]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["f64x8",3]]],[[["avx2",3],["i32x8",3]]],[[]],[[["avx2",3],["f32x8",3]]],[[]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["f32x8",3]]],[[["avx2",3],["f64x8",3]]],[[["avx2",3],["u64x8",3]]],[[["avx2",3],["i64x8",3]]],[[["u32x8",3],["avx2",3]]],[[["avx2",3],["f32x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["f64x8",3]]],[[]],[[["avx2",3],["i64x8",3]]],[[]],[[["mask",3],["avx2",3],["u64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["mask",3],["avx2",3],["i64x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]],[["avx2",3],["mask",3]]],[[["u32x8",3],["avx2",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["u64x8",3]],[["avx2",3],["mask",3]]],[[["u32x8",3],["avx2",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f32x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["i64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3],["avx2",3],["u64x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["i64x8",3]],[["avx2",3],["mask",3]]],[[["u32x8",3],["avx2",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f32x8",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["u32x8",3],["avx2",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["u64x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f32x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f64x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f64x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f32x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["u64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["i64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["avx2",3],["i32x8",3],["mask",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["u32x8",3],["avx2",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f32x8",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]],[["avx2",3],["mask",3]]],[[["avx2",3],["mask",3],["f64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3],["avx2",3],["i64x8",3]],[["avx2",3],["mask",3]]],[[["mask",3]],["mask",3]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3],["mask",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["i32x8",3]]],[[["avx2",3],["i32x8",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[]],null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],[["u32x8",3],["avx2",3]]],[[],[["avx2",3],["u64x8",3]]],[[],[["u32x8",3],["avx2",3]]],[[],[["avx2",3],["u64x8",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],[["avx2",3],["mask",3]]],[[],[["avx2",3],["mask",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[],["bool",15]],[[],[["avx2",3],["mask",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[["u32x8",3],["avx2",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["u32",15]]],[[]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[]],[[["u32",15]]],[[["u32",15]]],[[]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[["simdshuffleindices",8]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["i32",15]],["i32",15]],null,null,null,[[]],null,null,null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["simdcastiter",3]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["alignedmutiter",3]],[[],["option",4]],[[]],[[]],[[],["option",4]],[[],["option",4]],[[],["option",4]],[[]],[[]],[[]],[[]],[[["bool",15]]],[[["bool",15]]],[[["bool",15]]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,null,null,null,null,[[]],[[]],[[]],null,null,null,[[]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,[[],["bool",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],null,[[],["precisionpolicy",4]],[[]],[[["precisionpolicy",4]],["ordering",4]],null,[[]],[[]],[[]],[[]],[[["precisionpolicy",4]],["bool",15]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,[[]],[[]],[[["precisionpolicy",4]],[["option",4],["ordering",4]]],null,null,[[]],[[["usize",15]]],[[["usize",15]]],[[]],[[]],[[]],[[]],[[]],[[["i32",15]]],[[["i32",15]]],[[]],[[]],null,[[]],[[]],[[["isize",15]],["result",4]],[[["isize",15]],["result",4]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["usize",15]]],[[["usize",15]]],[[["isize",15]],["result",4]],[[["isize",15]],["result",4]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],null,null,[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[],["compensated",3]],[[]],null,[[["formatter",3]],["result",6]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],null,[[]],null,null,null,null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[],["extraprecision",3]],[[],["ultraperformance",3]],[[],["performance",3]],[[],["precision",3]],[[],["size",3]],[[],["reference",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[["extraprecision",3]],["ordering",4]],[[["ultraperformance",3]],["ordering",4]],[[["performance",3]],["ordering",4]],[[["precision",3]],["ordering",4]],[[["size",3]],["ordering",4]],[[["reference",3]],["ordering",4]],[[["extraprecision",3]],["bool",15]],[[["ultraperformance",3]],["bool",15]],[[["performance",3]],["bool",15]],[[["precision",3]],["bool",15]],[[["size",3]],["bool",15]],[[["reference",3]],["bool",15]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["extraprecision",3]],["bool",15]],[[["extraprecision",3]],[["option",4],["ordering",4]]],[[["ultraperformance",3]],[["option",4],["ordering",4]]],[[["performance",3]],[["option",4],["ordering",4]]],[[["precision",3]],[["option",4],["ordering",4]]],[[["size",3]],[["option",4],["ordering",4]]],[[["reference",3]],[["option",4],["ordering",4]]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],[[["simdfloatvector",8],["simd",8]],[["simdfloatvector",8],["simd",8]]],null,[[],["vf32",6]],[[],["vf64",6]],[[],["vu32",6]],[[],["vu64",6]],null,[[["vu64",6]]],null,null,[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[],["pcg32",3]],[[]],[[["pcg32",3]],["bool",15]],[[["formatter",3]],["result",6]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[["pcg32",3]],["bool",15]],[[["vu64",6]]],[[],["vu32",6]],[[["vu64",6]]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[],["splitmix64",3]],[[],["xoshiro128plus",3]],[[],["xoshiro256plus",3]],[[]],[[]],[[]],[[["splitmix64",3]],["bool",15]],[[["xoshiro128plus",3]],["bool",15]],[[["xoshiro256plus",3]],["bool",15]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[["splitmix64",3]],["bool",15]],[[["xoshiro128plus",3]],["bool",15]],[[["xoshiro256plus",3]],["bool",15]],[[["vu64",6]]],[[["vu64",6]]],[[["vu64",6]]],[[],["vu32",6]],[[],["vu64",6]],[[],["vu64",6]],[[],["vu64",6]],[[["vu64",6]]],[[["vu64",6]]],[[["vu64",6]]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[],["typeid",3]],[[],["typeid",3]]],"p":[[8,"SimdVectorBase"],[4,"SimdInstructionSet"],[8,"SimdBitwise"],[3,"Mask"],[8,"SimdShuffleIndices"],[8,"Simd"],[8,"SimdAssociatedVector"],[8,"SimdFloatVector"],[8,"SimdSignedVector"],[3,"VPtr"],[3,"BitMask"],[3,"VectorBuffer"],[8,"SimdBitsFrom"],[8,"SimdBitsInto"],[3,"Divider"],[3,"BranchfreeDivider"],[8,"SimdCastTo"],[8,"SimdCasts"],[8,"SimdVector"],[8,"SimdIntVector"],[8,"SimdFromBits"],[8,"SimdFromCast"],[8,"SimdIntoBits"],[8,"SimdUnsignedIntVector"],[3,"__m256i"],[3,"__m256"],[3,"__m256d"],[3,"__m128i"],[3,"__m128"],[3,"__m128d"],[3,"AVX1"],[3,"f32x8"],[3,"f64x8"],[3,"i32x8"],[3,"i64x8"],[3,"u32x8"],[3,"u64x8"],[3,"AVX2"],[8,"CastFrom"],[8,"IntoSimdIterator"],[3,"AlignedMut"],[3,"AlignedMutIter"],[3,"SimdSliceIter"],[3,"SimdCastIter"],[8,"SimdIteratorExt"],[4,"PrecisionPolicy"],[8,"SimdFloatVectorConsts"],[8,"Policy"],[8,"SimdVectorizedMath"],[8,"SimdVectorizedMathPolicied"],[3,"PolicyParameters"],[3,"Compensated"],[3,"ExtraPrecision"],[3,"UltraPerformance"],[3,"Performance"],[3,"Precision"],[3,"Size"],[3,"Reference"],[8,"SimdRng"],[3,"PCG32"],[3,"SplitMix64"],[3,"Xoshiro128Plus"],[3,"Xoshiro256Plus"]]},\
"thermite_complex":{"doc":"Complex Number Vectors","t":[3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11],"n":["Complex","acos","acosh","add","arg","asin","asinh","atan","atanh","borrow","borrow_mut","cast","cast_mask","cbrt","clone","clone_into","conj","cos","cosh","div","exp","expf","fdiv","finv","fmt","from","from_cast","from_cast_mask","from_polar","i","im","imag","into","inv","l1_norm","ln","log","mul","mul_add","neg","neg_i","new","norm","norm_sqr","one","powc","powf","re","real","scale","sin","sin_cos","sinh","splat","sqrt","sub","tan","tanh","to_owned","to_polar","try_from","try_into","type_id","unscale","with_policy","zero"],"q":["thermite_complex","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"d":["A vectorized (SoA) complex number in Cartesian form.","Computes the principal value of the inverse cosine of <code>self</code>","Computes the principal value of inverse hyperbolic cosine ","","Calculate the principal Arg of self.","Computes the principal value of the inverse sine of <code>self</code>.","Computes the principal value of inverse hyperbolic sine ","Computes the principal value of the inverse tangent of ","Computes the principal value of inverse hyperbolic ","","","","","Computes the principal value of the cube root of <code>self</code>.","","","Returns the complex conjugate. i.e. <code>re - i im</code>","Computes the cosine of <code>self</code>.","Computes the hyperbolic cosine of <code>self</code>.","","Computes <code>e^(self)</code>, where <code>e</code> is the base of the natural ","Raises a floating point number to the complex power <code>self</code>.","Returns <code>self/other</code> using floating-point operations.","Returns <code>1/self</code> using floating-point operations.","","","","","Convert a polar representation into a complex number.","Returns imaginary unit","Imaginary part","Create a new Complex <code>0+bi</code>","","Returns <code>1/self</code>","Returns the L1 norm <code>|re| + |im|</code>  the Manhattan distance","Computes the principal value of natural logarithm of <code>self</code>.","Returns the logarithm of <code>self</code> with respect to an ","","Returns <code>self * m + a</code>","","Return negative imaginary unit","","Calculate |self|","Returns the square of the norm","real(1)","Raises <code>self</code> to a complex power.","Raises <code>self</code> to a floating point power.","Real part","Create a new Complex <code>a+0i</code>","Multiplies <code>self</code> by the scalar <code>t</code>.","Computes the sine of <code>self</code>.","Computes sine and cosine of <code>self</code> together, improving ","Computes the hyperbolic sine of <code>self</code>.","Creates a new Complex with all lanes of <code>re</code> and <code>im</code> set to ","Computes the principal value of the square root of <code>self</code>.","","Computes the tangent of <code>self</code>.","Computes the hyperbolic tangent of <code>self</code>.","","Convert to polar form (r, theta), such that ","","","","Divides <code>self</code> by the scalar <code>t</code>.","","real(0)"],"i":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"f":[null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["formatter",3]],["result",6]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[]],[[],[["complex",3],["policy",8]]],[[]]],"p":[[3,"Complex"]]},\
"thermite_dispatch":{"doc":"","t":[23],"n":["dispatch"],"q":["thermite_dispatch"],"d":["Generates monomorphized backend <code>target_feature</code> function "],"i":[0],"f":[null],"p":[]},\
"thermite_hyperdual":{"doc":"","t":[6,6,3,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11],"n":["DualNumber","Hyperdual","HyperdualP","abs","acos","add","asin","atan","atan2","borrow","borrow_mut","cast","cast_mask","cbrt","clone","clone_into","div","du","exp","exp2","fmt","fract","from","from_cast","from_cast_mask","into","ln","map","map_dual","max","min","mul","mul_add","new","one","powf","powi","re","real","select","signum","sin_cos","sinh_cosh","sqrt","sub","tan","tanh","to_owned","try_from","try_into","type_id","zero"],"q":["thermite_hyperdual","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""],"d":["","","","","","","","","","","","","","","","","","Dual parts","","","","","","","","","","","","","","","","","","","","Real part","","","","","","","","","","","","","",""],"i":[0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"f":[null,null,null,[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],null,[[]],[[]],[[["formatter",3]],["result",6]],[[]],[[]],[[]],[[["mask",3]],["mask",3]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["i32",15]]],null,[[]],[[["mask",3]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[],["result",4]],[[],["result",4]],[[],["typeid",3]],[[]]],"p":[[3,"HyperdualP"]]},\
"thermite_special":{"doc":"","t":[8,8,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10],"n":["SimdVectorizedSpecialFunctions","SimdVectorizedSpecialFunctionsPolicied","bessel_j","bessel_j_p","bessel_y","bessel_y_p","beta","beta_p","digamma","digamma_p","gaussian","gaussian_integral","gaussian_integral_p","gaussian_p","hermite","hermite_p","hermitev","hermitev_p","jacobi","jacobi_p","legendre","legendre_p","lgamma","lgamma_p","tgamma","tgamma_p"],"q":["thermite_special","","","","","","","","","","","","","","","","","","","","","","","","",""],"d":["","","Computes the Bessel function of the first kind <code>J_n(x)</code> ","","Computes the Bessel function of the second kind <code>Y_n(x)</code> ","","Computes the Beta function <code>(x, y)</code>","","Computes the Digamma function <code>(x)</code>, the first derivative ","","Computes the generic Gaussian function:","Integrates the generic Gaussian function from <code>x0</code>(<code>self</code>) to ","","","Computes the n-th degree physicists Hermite polynomial ","","Computes the n-th degree physicists Hermite polynomial ","","Computes the m-th derivative of the n-th degree Jacobi ","","Computes the m-th associated n-th degree Legendre ","","Computes the natural log of the Gamma function (<code>ln((x))</code>","","Computes the Gamma function (<code>(z)</code>) for any real input, ",""],"i":[0,0,1,2,1,2,1,2,1,2,1,1,2,2,1,2,1,2,1,2,1,2,1,2,1,2],"f":[null,null,[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[]],[[["u32",15]]],[[["u32",15]]],[[]],[[]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[["u32",15]]],[[]],[[]],[[]],[[]]],"p":[[8,"SimdVectorizedSpecialFunctions"],[8,"SimdVectorizedSpecialFunctionsPolicied"]]}\
}');
if (window.initSearch) {window.initSearch(searchIndex)};